[
  {
    "reference": "A table for serialized DAGs.\n\n    serialized_dag table is a snapshot of DAG files synchronized by scheduler.\n    This feature is controlled by:\n\n    * ``[core] min_serialized_dag_update_interval = 30`` (s):\n      serialized DAGs are updated in DB when a file gets processed by scheduler,\n      to reduce DB write rate, there is a minimal interval of updating serialized DAGs.\n    * ``[dag_processor] refresh_interval = 300`` (s):\n      interval of deleting serialized DAGs in DB when the files are deleted, suggest\n      to use a smaller interval such as 60\n    * ``[core] compress_serialized_dags``:\n      whether compressing the dag data to the Database.\n\n    It is used by webserver to load dags\n    because reading from database is lightweight compared to importing from files,\n    it solves the webserver scalability issue.",
    "prediction": "The SerializedDAG class represents the serialized DAG in the database. Serialized DAG\nis not directly used in the Airflow application. Serialized DAGs are used by the DAG\nmodel to store information about the DAGs.\n\nSerialized DAGs are used by the Airflow application for the following purposes:\n\n    1. When a DAG is run, the serialized DAG is used to generate a DAG run object, which\n       contains the information about the DAG run, including the dag_id and the dag\n       version. This information is used by the DAG run object to determine the DAG\n       version, and to generate the dag run. The serialized DAG is also used by the DAG\n       run object to generate the DAG run code, which is used by the DAG run to\n       execute the DAG.\n\n    2. When a DAG is run, the serialized DAG is used to generate a DAG run object, which\n       contains the information about the DAG run, including the dag_id and the dag\n       version. This information is used by the DAG run object to determine the DAG\n       version, and to generate the dag run. The serialized DAG is also used by the DAG\n       run object to generate the DAG run code, which is used by the DAG run"
  },
  {
    "reference": "Lazy-loaded cached dictionary.\n\n    Dictionary, which in case you set callable, executes the passed callable with `key` attribute\n    at first use - and returns and caches the result.",
    "prediction": "\"\"\"\nfrom typing import Any, Dict, Iterable, List, Mapping, TypeVar, Union, cast, get_type_hints\n\nfrom.base import BaseLazyDict\nfrom.cached_property import CachedProperty\nfrom.lazyproperty import LazyProperty\nfrom.typing import AnyStr, AnyStrOrCallable, AnyStrOrIterable, AnyStrOrTuple, AnyStrOrTupleOrCallable, AnyStrOrTuples, AnyStrOrIterableOrCallable, AnyStrOrTupleOrIterable, AnyStrOrIterableOrTuples, AnyStrOrTupleOrIterableOrCallable, AnyStrOrIterableOrTuplesOrCallable, AnyStrOrTupleOrIterableOrTuple, AnyStrOrTupleOrIterableOrTupleOrCallable, AnyStrOrTupleOrIterableOrTupleOrIterable, AnyStrOrTupleOrIterableOrTupleOrIterableOrCallable, AnyStrOrTupleOrIterableOrTupleOrIterableOrTuple, AnyStrOrTupleOrIterableOrTupleOrIterableOrTupleOrCallable, AnyStrOrTupleOrIterableOrTupleOrIterableOrTupleOrIterable, AnyStrOrTupleOrIterableOrTupleOrIterableOrTupleOrIterableOrCallable, AnyStrOrTupleOrIterableOrTupleOrIterableOrTupleOrIterableOrIterable, AnyStrOrTupleOr"
  },
  {
    "reference": "A dag (directed acyclic graph) is a collection of tasks with directional dependencies.\n\n    A dag also has a schedule, a start date and an end date (optional).  For each schedule,\n    (say daily or hourly), the DAG needs to run each individual tasks as their dependencies\n    are met. Certain tasks have the property of depending on their own past, meaning that\n    they can't run until their previous schedule (and upstream tasks) are completed.\n\n    DAGs essentially act as namespaces for tasks. A task_id can only be\n    added once to a DAG.\n\n    Note that if you plan to use time zones all the dates provided should be pendulum\n    dates. See :ref:`timezone_aware_dags`.\n\n    .. versionadded:: 2.4\n        The *schedule* argument to specify either time-based scheduling logic\n        (timetable), or asset-driven triggers.\n\n    .. versionchanged:: 3.0\n        The default value of *schedule* has been changed to *None* (no schedule).\n        The previous default was ``timedelta(days=1)``.\n\n    :param dag_id: The id of the DAG; must consist exclusively of alphanumeric\n        characters, dashes, dots and underscores (all ASCII)\n    :param description: The description for the DAG to e.g. be shown on the webserver\n    :param schedule: If provided, this defines the rules according to which DAG\n        runs are scheduled. Possible values include a cron expression string,\n        timedelta object, Timetable, or list of Asset objects.\n        See also :doc:`/howto/timetable`.\n    :param start_date: The timestamp from which the scheduler will\n        attempt to backfill. If this is not provided, backfilling must be done\n        manually with an explicit time range.\n    :param end_date: A date beyond which your DAG won't run, leave to None\n        for open-ended scheduling.\n    :param template_searchpath: This list of folders (non-relative)\n        defines where jinja will look for your templates. Order matters.\n        Note that jinja/airflow includes the path of your DAG file by\n        default\n    :param template_undefined: Template undefined type.\n    :param user_defined_macros: a dictionary of macros that will be exposed\n        in your jinja templates. For example, passing ``dict(foo='bar')``\n        to this argument allows you to ``{{ foo }}`` in all jinja\n        templates related to this DAG. Note that you can pass any\n        type of object here.\n    :param user_defined_filters: a dictionary of filters that will be exposed\n        in your jinja templates. For example, passing\n        ``dict(hello=lambda name: 'Hello %s' % name)`` to this argument allows\n        you to ``{{ 'world' | hello }}`` in all jinja templates related to\n        this DAG.\n    :param default_args: A dictionary of default parameters to be used\n        as constructor keyword parameters when initialising operators.\n        Note that operators have the same hook, and precede those defined\n        here, meaning that if your dict contains `'depends_on_past': True`\n        here and `'depends_on_past': False` in the operator's call\n        `default_args`, the actual value will be `False`.\n    :param params: a dictionary of DAG level parameters that are made\n        accessible in templates, namespaced under `params`. These\n        params can be overridden at the task level.\n    :param max_active_tasks: the number of task instances allowed to run\n        concurrently\n    :param max_active_runs: maximum number of active DAG runs, beyond this\n        number of DAG runs in a running state, the scheduler won't create\n        new active DAG runs\n    :param max_consecutive_failed_dag_runs: (experimental) maximum number of consecutive failed DAG runs,\n        beyond this the scheduler will disable the DAG\n    :param dagrun_timeout: Specify the duration a DagRun should be allowed to run before it times out or\n        fails. Task instances that are running when a DagRun is timed out will be marked as skipped.\n    :param sla_miss_callback: DEPRECATED - The SLA feature is removed in Airflow 3.0, to be replaced with a new implementation in 3.1\n    :param catchup: Perform scheduler catchup (or only run latest)? Defaults to False\n    :param on_failure_callback: A function or list of functions to be called when a DagRun of this dag fails.\n        A context dictionary is passed as a single parameter to this function.\n    :param on_success_callback: Much like the ``on_failure_callback`` except\n        that it is executed when the dag succeeds.\n    :param access_control: Specify optional DAG-level actions, e.g.,\n        \"{'role1': {'can_read'}, 'role2': {'can_read', 'can_edit', 'can_delete'}}\"\n        or it can specify the resource name if there is a DAGs Run resource, e.g.,\n        \"{'role1': {'DAG Runs': {'can_create'}}, 'role2': {'DAGs': {'can_read', 'can_edit', 'can_delete'}}\"\n    :param is_paused_upon_creation: Specifies if the dag is paused when created for the first time.\n        If the dag exists already, this flag will be ignored. If this optional parameter\n        is not specified, the global config setting will be used.\n    :param jinja_environment_kwargs: additional configuration options to be passed to Jinja\n        ``Environment`` for template rendering\n\n        **Example**: to avoid Jinja from removing a trailing newline from template strings ::\n\n            DAG(\n                dag_id=\"my-dag\",\n                jinja_environment_kwargs={\n                    \"keep_trailing_newline\": True,\n                    # some other jinja2 Environment options here\n                },\n            )\n\n        **See**: `Jinja Environment documentation\n        <https://jinja.palletsprojects.com/en/2.11.x/api/#jinja2.Environment>`_\n\n    :param render_template_as_native_obj: If True, uses a Jinja ``NativeEnvironment``\n        to render templates as native Python types. If False, a Jinja\n        ``Environment`` is used to render templates as string values.\n    :param tags: List of tags to help filtering DAGs in the UI.\n    :param owner_links: Dict of owners and their links, that will be clickable on the DAGs view UI.\n        Can be used as an HTTP link (for example the link to your Slack channel), or a mailto link.\n        e.g: {\"dag_owner\": \"https://airflow.apache.org/\"}\n    :param auto_register: Automatically register this DAG when it is used in a ``with`` block\n    :param fail_fast: Fails currently running tasks when task in DAG fails.\n        **Warning**: A fail fast dag can only have tasks with the default trigger rule (\"all_success\").\n        An exception will be thrown if any task in a fail fast dag has a non default trigger rule.\n    :param dag_display_name: The display name of the DAG which appears on the UI.",
    "prediction": "Docstring explaining what the method does.\n\n    :param dagrun: DagRun object\n    :param success: True if the task succeeded, False otherwise\n    :param reason: Reason for the task failure\n    :param session: Session object\n    \"\"\"\n        self.log.debug(\"DAG run: %s\", dagrun)\n        self.log.debug(\"DAG run: %s\", dagrun.dag_id)\n        if success:\n            self.log.debug(\"DAG run: %s\", dagrun.dag_id)\n            self.log.debug(\"DAG run: %s\", dagrun.dag_id)\n            self.log.debug(\"DAG run: %s\", dagrun.dag_id)\n            self.log.debug(\"DAG run: %s\", dagrun.dag_id)\n            self.log.debug(\"DAG run: %s\", dagrun.dag_id)\n            self.log.debug(\"DAG run: %s\", dagrun.dag_id)\n            self.log.debug(\"DAG run: %s\", dagrun.dag_id)\n            self.log.debug(\"DAG run: %s\", dagrun.dag_id)\n            self.log.debug(\"DAG run: %s\", dagrun.dag_id)\n            self.log.debug"
  },
  {
    "reference": "States that a Task Instance can be in that indicate it is not yet in a terminal or running state.",
    "prediction": "Intermediate TI state. Used for logging and to determine if a job is active or not.\n\"\"\""
  },
  {
    "reference": "Exception raised when a model populates data interval fields incorrectly.\n\n    The data interval fields should either both be None (for runs scheduled\n    prior to AIP-39), or both be datetime (for runs scheduled after AIP-39 is\n    implemented). This is raised if exactly one of the fields is None.",
    "prediction": "This exception is raised when Airflow detects that the interval\nspecified in the"
  },
  {
    "reference": "Raise when the task should be re-scheduled at a later time.\n\n    :param reschedule_date: The date when the task should be rescheduled",
    "prediction": "AirflowRescheduleException is used to raise an exception when a scheduled\ntask is rescheduled. This exception is raised by Airflow when a rescheduled\ntask is detected in the DAG, and should be caught by the Airflow scheduler.\n\"\"\""
  },
  {
    "reference": "Manages all provider distributions.\n\n    This is a Singleton class. The first time it is\n    instantiated, it discovers all available providers in installed packages.",
    "prediction": "This is the entry point for the initialization of the Airflow providers.\n\nIt is called by the Airflow entrypoint and is responsible for initializing the Airflow providers.\n\nThis function can be used as an entrypoint for Airflow providers, and it can be called from within the Airflow\ncodebase.\n\"\"\"\n    def initialize_providers(self) -> None:\n        \"\"\"Initialize Airflow providers.\"\"\"\n        self._discover_all_providers_from_packages()\n        self._verify_all_providers_all_compatible()\n        self._provider_dict = dict(sorted(self._provider_dict.items()))\n        self._discover_hooks()\n        self._hook_provider_dict = dict(sorted(self._hook_provider_dict.items()))\n        self._discover_extra_links()\n        self._discover_config()\n        self._discover_notification_info()\n        self._discover_trigger_info()\n        self._discover_airflow_core_hooks()\n        self._discover_asset_uri_resources()\n        self._discover_taskflow_decorators()\n        self._discover_secrets_backends()\n        self._discover_executors()\n        self._discover_notifications()\n        self._discover_airflow_core_hooks()\n        self._discover_airflow_core_hooks()\n        self._discover_airflow_core_hooks()\n        self._discover_airflow_core_hooks()\n        self._discover_airflow_core_hooks()"
  },
  {
    "reference": "Table defining different owner attributes.\n\n    For example, a link for an owner that will be passed as a hyperlink to the \"DAGs\" view.",
    "prediction": "DagOwnerAttributes represents the attributes that are unique to a Dag owner\n\"\"\""
  },
  {
    "reference": "Store info needed to create XComArg.\n\n    We can't turn it in to a XComArg until we've loaded _all_ the tasks, so when\n    deserializing an operator, we need to create something in its place, and\n    post-process it in ``deserialize_dag``.",
    "prediction": "The XComRef class is used to reference XComs within a DAG.\n\nTo reference an XCom within a DAG, a user would"
  },
  {
    "reference": "Holds modifications to be applied when writing out the config.\n\n    :param rename: Mapping from (old_section, old_option) to (new_section, new_option)\n    :param remove: Set of (section, option) to remove\n    :param default_updates: Mapping from (section, option) to new default value",
    "prediction": "This module contains the ConfigModifications class, which defines a set of methods\nto modify a configuration file. The ConfigModifications class is used to modify a\nconfiguration file, and the methods in this class are used to modify the file.\n\nThe ConfigModifications class has the following methods:\n\n    - add_rename: Adds a new rename to the configuration file.\n    - add_remove: Adds a new remove to the configuration file.\n    - add_default_update: Adds a new default update to the configuration file.\n\nThe ConfigModifications class also defines the following constants:\n\n    - DEFAULT_UPDATE: The default update to the configuration file.\n\nThe ConfigModifications class is designed to be used in conjunction with the\n`configmod` Python module. The configmod module provides methods for modifying a\nconfiguration file. The ConfigModifications class is used to modify the configuration\nfile using the configmod module.\n\nThe ConfigModifications class is intended to be used by developers who want to\nmodify a configuration file. It provides a way to modify the configuration file\nusing the configmod module.\n\nThe ConfigModifications class is not intended to be used by end users. It is\nintended to be used by developers who want to modify a configuration file.\n\nThe ConfigModifications class"
  },
  {
    "reference": "Timetable that triggers DAG runs according to a cron expression.\n\n    This is different from ``CronDataIntervalTimetable``, where the cron\n    expression specifies the *data interval* of a DAG run. With this timetable,\n    the data intervals are specified independently from the cron expression.\n    Also for the same reason, this timetable kicks off a DAG run immediately at\n    the start of the period (similar to POSIX cron), instead of needing to wait\n    for one data interval to pass.\n\n    Don't pass ``@once`` in here; use ``OnceTimetable`` instead.\n\n    :param cron: cron string that defines when to run\n    :param timezone: Which timezone to use to interpret the cron string\n    :param interval: timedelta that defines the data interval start. Default 0.\n\n    *run_immediately* controls, if no *start_time* is given to the DAG, when\n    the first run of the DAG should be scheduled. It has no effect if there\n    already exist runs for this DAG.\n\n    * If *True*, always run immediately the most recent possible DAG run.\n    * If *False*, wait to run until the next scheduled time in the future.\n    * If passed a ``timedelta``, will run the most recent possible DAG run\n      if that run's ``data_interval_end`` is within timedelta of now.\n    * If *None*, the timedelta is calculated as 10% of the time between the\n      most recent past scheduled time and the next scheduled time. E.g. if\n      running every hour, this would run the previous time if less than 6\n      minutes had past since the previous run time, otherwise it would wait\n      until the next hour.",
    "prediction": "This class provides a timed trigger that can be used to schedule jobs at a specified time or time interval.\nThis trigger can be used in Airflow to schedule jobs at a specific time or time interval.\nThe trigger is designed to be used with the cron expression format.\nThe trigger can be configured with the following attributes:\n\n- Cron: The cron expression to be used for scheduling the job.\n- Timezone: The time zone to be used when parsing the cron expression.\n- Interval: The interval at which the job should be scheduled, expressed as a datetime.timedelta or relativedelta.\n- Run immediately: Whether the job should be scheduled immediately.\n- Time: The time at which the job should be scheduled, expressed as a datetime.datetime.\n- Day of week: The day of the week at which the job should be scheduled, expressed as an integer (0-7).\n- Month: The month at which the job should be scheduled, expressed as an integer (0-11).\n- Year: The year at which the job should be scheduled, expressed as an integer (0-99).\n- Hour: The hour at which the job should be scheduled, expressed as an integer (0-23).\n- Minute: The minute at which the job should be scheduled, expressed as an"
  },
  {
    "reference": "Collection of active assets.\n\n    An asset is considered active if it is declared by the user in any DAG files.\n    AssetModel entries that are not active (also called orphaned in some parts\n    of the code base) are still kept in the database, but have their corresponding\n    entries in this table removed. This ensures we keep all possible history on\n    distinct assets (those with non-matching name-URI pairs), but still ensure\n    *name and URI are each unique* within active assets.",
    "prediction": "Asset Active is a column which is created when an Asset Model is created.\nThis column is used to track if an Asset is active or not.\n\nThe name and uri columns are required and the asset column is optional.\nIf the asset column is not provided then the asset column will be filled with the asset name\nand uri respectively.\n\nWhen a new Asset Active is created then the asset column will be filled with the asset name\nand uri respectively.\n\nWhen an Asset Active is updated then the asset column will be updated with the asset name\nand uri respectively.\n\nWhen an Asset Active is deleted then the asset column will be updated with the asset name\nand uri respectively.\n\"\"\""
  },
  {
    "reference": "A base class for dependency contexts.\n\n    Specifies which dependencies should be evaluated in the context for a task\n    instance to satisfy the requirements of the context. Also stores state\n    related to the context that can be used by dependency classes.\n\n    For example there could be a SomeRunContext that subclasses this class which has\n    dependencies for:\n\n    - Making sure there are slots available on the infrastructure to run the task instance\n    - A task-instance's task-specific dependencies are met (e.g. the previous task\n      instance completed successfully)\n    - ...\n\n    :param deps: The context-specific dependencies that need to be evaluated for a\n        task instance to run in this execution context.\n    :param flag_upstream_failed: This is a hack to generate the upstream_failed state\n        creation while checking to see whether the task instance is runnable. It was the\n        shortest path to add the feature. This is bad since this class should be pure (no\n        side effects).\n    :param ignore_all_deps: Whether or not the context should ignore all ignorable\n        dependencies. Overrides the other ignore_* parameters\n    :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs (e.g. for\n        Backfills)\n    :param wait_for_past_depends_before_skipping: Wait for past depends before marking the ti as skipped\n    :param ignore_in_retry_period: Ignore the retry period for task instances\n    :param ignore_in_reschedule_period: Ignore the reschedule period for task instances\n    :param ignore_unmapped_tasks: Ignore errors about mapped tasks not yet being expanded\n    :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past and\n        trigger rule\n    :param ignore_ti_state: Ignore the task instance's previous failure/success\n    :param finished_tis: A list of all the finished task instances of this run",
    "prediction": "A context for evaluating dependencies for a DAG. \n\nAttributes:\n    deps: A set of TaskId or Task instances to evaluate against.\n    flag_upstream_failed: A flag that is set when a task fails to run upstream tasks.\n    ignore_all_deps: A flag that indicates whether all the dependencies should be ignored.\n    ignore_depends_on_past: A flag that indicates whether to ignore the dependencies on the past.\n    wait_for_past_depends_before_skipping: A flag that indicates whether to wait for the past depends before skipping.\n    ignore_in_retry_period: A flag that indicates whether to ignore the task if it fails to retry in the retry period.\n    ignore_in_reschedule_period: A flag that indicates whether to ignore the task if it fails to reschedule in the reschedule period.\n    ignore_task_deps: A flag that indicates whether to ignore the task if it depends on other tasks.\n    ignore_ti_state: A flag that indicates whether to ignore the task if it has an invalid ti state.\n    ignore_unmapped_tasks: A flag that indicates whether to ignore the task if it is unmapped.\n    finished_tis: A list of finished tasks of this DAG and logical_date.\n    description: A description of the task if one exists.\n\"\"\"\n\nDoc"
  },
  {
    "reference": "Request for DAG File Parsing.\n\n    This is the request that the manager will send to the DAG parser with the dag file and\n    any other necessary metadata.",
    "prediction": "This class is used to pass request data to the underlying DagRun API"
  },
  {
    "reference": "Serializer for responses to bulk entity operations.\n\n    This represents the results of create, update, and delete actions performed on entity in bulk.\n    Each action (if requested) is represented as a field containing details about successful keys and any encountered errors.\n    Fields are populated in the response only if the respective action was part of the request, else are set None.",
    "prediction": "This class holds the bulk action responses.  These are used to represent the\n    bulk action responses to the API client.\n\n    A bulk action response is an API response that is generated when a bulk\n    operation is triggered.  Bulk actions are triggered when the user makes a\n    bulk create or bulk update request.\n\n    Bulk actions are triggered by the bulk create and bulk update APIs.  These\n    are both triggered by the bulk create API.  The bulk update API is triggered\n    by the bulk update API.  The bulk update API is triggered by the bulk update\n    API.\n\n    Bulk actions are triggered when the user makes a bulk create or bulk update\n    request.  Bulk actions are also triggered when a bulk update request is\n    triggered.  Bulk actions are triggered when a bulk update request is\n    triggered.  Bulk actions are triggered when a bulk update request is\n    triggered.  Bulk actions are triggered when a bulk update request is\n    triggered.\n\n    Bulk actions are triggered when a bulk update request is triggered.  Bulk\n    actions are triggered when a bulk update request is triggered.  Bulk actions\n    are triggered when a bulk update request is triggered.  Bulk actions are\n    triggered when a bulk"
  },
  {
    "reference": "TriggerRunnerSupervisor is responsible for monitoring the subprocess and marshalling DB access.\n\n    This class (which runs in the main process) is responsible for querying the DB, sending RunTrigger\n    workload messages to the subprocess, and collecting results and updating them in the DB.",
    "prediction": "A TriggerRunner is responsible for running a TriggerJob.\n\nTriggerRunner is a base class that implements the functionality of a TriggerRunner. Subclasses of\nTriggerRunner are responsible for the specific functionality of a TriggerRunner. Subclasses are\nresponsible for implementing the following methods:\n\n    - _service_subprocess(self, timeout: int)\n    - _run_in_process(self)\n    - run(self) -> None\n\nTriggerRunner is a base class that implements the functionality of a TriggerRunner. Subclasses of\nTriggerRunner are responsible for the specific functionality of a TriggerRunner. Subclasses are\nresponsible for implementing the following methods:\n\n    - _service_subprocess(self, timeout: int)\n    - _run_in_process(self)\n    - run(self) -> None\n\nTriggerRunner is a base class that implements the functionality of a TriggerRunner. Subclasses of\nTriggerRunner are responsible for the specific functionality of a TriggerRunner. Subclasses are\nresponsible for implementing the following methods:\n\n    - _service_subprocess(self, timeout: int)\n    - _run_in_process(self)\n    - run(self) -> None\n\nTriggerRunner is a base class that implements the functionality of a TriggerRunner. Subclasses of\nTriggerRunner are responsible for the specific functionality of a"
  },
  {
    "reference": "An implementation of Stats.Timer() which records the result in the OTel Metrics Map.\n\n    OpenTelemetry does not have a native timer, we will store the values as a Gauge.\n\n    :param name: The name of the timer.\n    :param tags: Tags to append to the timer.",
    "prediction": "This is the base class for all timer objects. It provides a base class for all\n    timer objects that will be added to the otel_metrics_map.\n    \"\"\""
  },
  {
    "reference": "Ensures that the task instance's state is in a given set of valid states.\n\n    :param valid_states: A list of valid states that a task instance can have to meet\n        this dependency.\n    :return: whether or not the task instance's state is valid",
    "prediction": "The ValidStateDep class is a task instance dependency that checks if the task instance is in a valid state. If the task instance is in a valid state, the dependency passes. If the task instance is in an invalid state, the dependency fails.\n\nThis task instance dependency can be used to check if the task instance is in a valid state before executing the task. It can also be used to check if the task instance is in a valid state after executing the task, to ensure that the task instance has completed successfully.\n\nThe ValidStateDep class is useful for checking if the task instance is in a valid state before executing the task. It can also be used to check if the task instance is in a valid state after executing the task, to ensure that the task instance has completed successfully.\n\nThe ValidStateDep class is a task instance dependency that checks if the task instance is in a valid state. If the task instance is in a valid state, the dependency passes. If the task instance is in an invalid state, the dependency fails.\n\nThis task instance dependency can be used to check if the task instance is in a valid state before executing the task. It can also be used to check if the task instance is in a valid state after executing the task, to ensure that the"
  },
  {
    "reference": "The resources required by an operator.\n\n    Resources that are not specified will use the default values from the airflow config.\n\n    :param cpus: The number of cpu cores that are required\n    :param ram: The amount of RAM required\n    :param disk: The amount of disk space required\n    :param gpus: The number of gpu units that are required",
    "prediction": "\"\"\"\nfrom operator import getitem\n\nfrom airflow.utils import task_run_context\nfrom airflow.utils.decorators import task_cached_property\nfrom airflow.utils.task_group import TaskGroupContext\n\nfrom airflow.models import AirflowTaskInstance\n\nfrom airflow.utils.state import State\n\nfrom airflow.providers.amazon.aws.sensors.s3_bucket_exists import S3BucketExistsSensor\n\nfrom airflow.utils.dag_run import DagRun\n\nfrom airflow.utils.task_context import get_context\n\nfrom airflow.utils.scheduling import schedule_every\n\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\n\nfrom airflow.providers.amazon.aws.hooks.sns import SNSHook\n\nfrom airflow.providers.amazon.aws.sensors.sns import SNSSensor\n\nfrom airflow.providers.amazon.aws.sensors.sqs import SQSHook\n\nfrom airflow.providers.amazon.aws.utils import (\n    get_iam_role,\n    get_iam_role_policy,\n    get_iam_policy,\n    get_iam_policy_document,\n)\nfrom airflow.providers.amazon.aws.utils import get_iam_role_session\n\nfrom airflow.providers.amazon.aws.utils import get_iam_role_session_expiration\n\nfrom airflow.providers.amazon.aws.utils import get_iam_role_session_expiration import (\n    get_iam_role_session_expiration,\n)\nfrom airflow.providers.amazon.aws.utils import get_"
  },
  {
    "reference": "Timetable that schedules data intervals with a cron expression.\n\n    This corresponds to ``schedule=<cron>``, where ``<cron>`` is either\n    a five/six-segment representation, or one of ``cron_presets``.\n\n    The implementation extends on croniter to add timezone awareness. This is\n    because croniter works only with naive timestamps, and cannot consider DST\n    when determining the next/previous time.\n\n    Don't pass ``@once`` in here; use ``OnceTimetable`` instead.",
    "prediction": "This is a generic abstract base class for data intervals. It defines the\n    following methods:\n\n    - :meth:`__init__` : initialize the data interval.\n    - :meth:`__repr__` : override to print the data interval.\n    - :meth:`__str__` : override to print the data interval.\n    - :meth:`__iter__` : override to return the data interval.\n    - :meth:`__getitem__` : override to return the data interval at a specific\n      index.\n    - :meth:`__setitem__` : override to update the data interval at a specific\n      index.\n    - :meth:`__contains__` : override to check if a specific value is present\n      in the data interval.\n    - :meth:`__len__` : override to return the length of the data interval.\n    - :meth:`__eq__` : override to check if two data intervals are equal.\n    - :meth:`__ne__` : override to check if two data intervals are not equal.\n    - :meth:`__add__` : override to add two data intervals.\n    - :meth:`__sub__` : override to subtract two data intervals"
  },
  {
    "reference": "Base Trigger class.\n\n    Triggers are a workload that run in an asynchronous event loop shared with\n    other Triggers, and fire off events that will unpause deferred Tasks,\n    start linked DAGs, etc.\n\n    They are persisted into the database and then re-hydrated into a\n    \"triggerer\" process, where many are run at once. We model it so that\n    there is a many-to-one relationship between Task and Trigger, for future\n    deduplication logic to use.\n\n    Rows will be evicted from the database when the triggerer detects no\n    active Tasks/DAGs using them. Events are not stored in the database;\n    when an Event is fired, the triggerer will directly push its data to the\n    appropriate Task/DAG.",
    "prediction": "This is the base class for triggers that are used in Airflow.  It is not meant\nto be used directly by end users.  Instead, it is meant to be used by the trigger\nAPI, which will provide an interface for creating triggers.\n\nA trigger is associated with a task instance, which represents a DAG run.  The\ntrigger can be fired by the scheduler at any time, and when it is fired, it will\nattempt to run the associated task instance.\n\nA trigger can also be fired manually by the scheduler.  This is typically done\nwhen the DAG is running, and the trigger is set to \"manual\" (or \"on-demand\" if\nusing the Python API).  This allows the user to trigger a run manually, which\ncould be useful for debugging or when trying to run a single task instance.\n\nA trigger can also be fired by a triggerer job, which is a separate Airflow job\nthat is run by a triggerer process (see :ref:`triggerer-job`).  This is useful\nwhen the trigger is being run by a triggerer job, such as when the trigger is\nused to kick off a triggerer job that is run by a triggerer process.\n\nThe trigger is not executed until it"
  },
  {
    "reference": "Lazily build information from the serialized DAG structure.\n\n    An object that will present \"enough\" of the DAG like interface to update DAG db models etc, without having\n    to deserialize the full DAG and Task hierarchy.",
    "prediction": "This class stores a serialized DAG.\n\nThis class should only be used when deserializing DAGs. It will be used to serialize DAGs as well, but it is not a good idea to do so.\n\nThis class is used by the :class:`~airflow.models.serialized_dag.SerializedDAG` class to deserialize DAGs. This class is not intended to be used outside of the serialization logic.\n\nThis class is used by the :class:`~airflow.models.serialized_dag.SerializedDAG` class to serialize DAGs. It is not intended to be used outside of the serialization logic.\n\nThis class is used by the :class:`~airflow.models.serialized_dag.SerializedDAG` class to deserialize DAGs. This class is not intended to be used outside of the serialization logic.\n\nThis class is used by the :class:`~airflow.models.serialized_dag.SerializedDAG` class to serialize DAGs. It is not intended to be used outside of the serialization logic.\n\"\"\""
  },
  {
    "reference": "An extension of FileHandler, advises the Kernel to not cache the file in PageCache when it is written.\n\n    While there is nothing wrong with such cache (it will be cleaned when memory is needed), it\n    causes ever-growing memory usage when scheduler is running as it keeps on writing new log\n    files and the files are not rotated later on. This might lead to confusion for our users,\n    who are monitoring memory usage of Scheduler - without realising that it is harmless and\n    expected in this case.\n\n    See https://github.com/apache/airflow/issues/14924\n\n    Adding the advice to Kernel might help with not generating the cache memory growth in the first place.",
    "prediction": "A subclass of FileHandler that delegates to a FileHandler that doesn't cache"
  },
  {
    "reference": "SchedulerJobRunner runs for a specific time interval and schedules jobs that are ready to run.\n\n    It figures out the latest runs for each task and sees if the dependencies\n    for the next schedules are met.\n    If so, it creates appropriate TaskInstances and sends run commands to the\n    executor. It does this for each task in each DAG and repeats.\n\n    :param num_runs: The number of times to run the scheduling loop. If you\n        have a large number of DAG files this could complete before each file\n        has been parsed. -1 for unlimited times.\n    :param scheduler_idle_sleep_time: The number of seconds to wait between\n        polls of running processors\n    :param log: override the default Logger",
    "prediction": "\"\"\"\n    self.log.debug(\n        \"Num starved tasks total: %s, num starved dag runs total: %s, num starved tasks per dag run total: %s\",\n        num_starved_tasks_total,\n        num_starved_dags,\n        num_starved_tasks_task_dagrun_concurrency,\n    )\n\n    # Get the number of starved tasks\n    num_starved_tasks = len(starved_tasks)\n\n    # Get the number of starved tasks per dag run\n    num_starved_tasks_per_dag_run = len(starved_tasks_task_dagrun_concurrency)\n\n    if num_starved_tasks > max_tis:\n        self.log.info(\n            \"Not executing %s since there are %s tasks to execute\",\n            task_instance,\n            num_starved_tasks,\n        )\n        return []\n\n    if num_starved_tasks_per_dag_run > max_tis:\n        self.log.info(\n            \"Not executing %s since there are %s tasks to execute\",\n            task_instance,\n            num_starved_tasks_per_dag_run,\n        )\n        return []\n\n    if num_starved_tasks_per_dag_run > num_starved_tasks:\n        self.log.info(\n            \"Not executing %s since there are %s"
  },
  {
    "reference": "Handle all tracing requirements such as getting the tracer, and starting a new span.\n\n    When OTEL is enabled, the Trace class will be replaced by this class.",
    "prediction": "\"\"\"\nclass TraceContextTextMapPropagator:\n    \n\n    @classmethod\n    def _get_context(cls, context: Context) -> Context:\n        \"\"\"Get the context from the provided context.\"\"\"\n        # We can use the same context for different spans if the span context is valid.\n        if context.is_valid():\n            return context\n        else:\n            # We can't use the same context for different spans if the span context is invalid.\n            # We need to create a new context to use as the span context.\n            context = trace.set_span_in_context(NonRecordingSpan(context))\n            return context\n\n    @classmethod\n    def _get_span_context(cls, span_context: SpanContext) -> Context:\n        \"\"\"Get the span context from the provided span context.\"\"\"\n        # We can use the same span context for different spans if the span context is valid.\n        if span_context.is_valid():\n            return span_context\n        else:\n            # We can't use the same span context for different spans if the span context is invalid.\n            # We need to create a new span context to use as the span context.\n            span_context = trace.set_span_in_context(NonRecordingSpan(span_context))\n            return span_context\n\n    @classmethod"
  }
]