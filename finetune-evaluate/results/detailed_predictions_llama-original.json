[
  {
    "reference": "A table for serialized DAGs.\n\n    serialized_dag table is a snapshot of DAG files synchronized by scheduler.\n    This feature is controlled by:\n\n    * ``[core] min_serialized_dag_update_interval = 30`` (s):\n      serialized DAGs are updated in DB when a file gets processed by scheduler,\n      to reduce DB write rate, there is a minimal interval of updating serialized DAGs.\n    * ``[dag_processor] refresh_interval = 300`` (s):\n      interval of deleting serialized DAGs in DB when the files are deleted, suggest\n      to use a smaller interval such as 60\n    * ``[core] compress_serialized_dags``:\n      whether compressing the dag data to the Database.\n\n    It is used by webserver to load dags\n    because reading from database is lightweight compared to importing from files,\n    it solves the webserver scalability issue.",
    "prediction": "This module provides a serialized DAG model for Airflow. It is not intended to be used directly by Airflow. The serialized DAG is not persisted in the database, but is serialized into JSON data.\n\nThe serialized DAG model is used by Airflow to store the DAG in the database. It is used to retrieve the DAG and the DAG version from the database.\n\nThe serialized DAG model contains the following fields:\n    - dag_id: the DAG ID\n    - dag_hash: the DAG hash\n    - created_at: the date the DAG was created in UTC\n    - last_updated: the date the DAG was last updated in UTC\n    - dag_model: the DAG model\n    - dag_version: the DAG version\n    - dag_runs: the DAG runs\n\nThe serialized DAG model also contains the following fields that are not used in the database:\n    - dag_data: the serialized DAG data\n    - dag_data_compressed: the serialized DAG data compressed\n    - load_op_links: whether the DAG has load operator links\n\nThe serialized DAG model can be serialized into JSON data using the following method:\n    - SerializedDagModel.serialize()\n    - SerializedDagModel.to_dict()\n    - SerializedDagModel.to_json()\n    - SerializedDagModel"
  },
  {
    "reference": "Lazy-loaded cached dictionary.\n\n    Dictionary, which in case you set callable, executes the passed callable with `key` attribute\n    at first use - and returns and caches the result.",
    "prediction": "LazyDictWithCache is a mutable dictionary that allows caching values\nfrom a callable function.  The callable function is called once,\nwhen the dictionary is first created, and again for each\nkey that is subsequently accessed.  This allows the callable\nto compute the cached value for a given key, and cache the result.\nIf the callable returns a non-callback value, the result is\nreturned immediately, without caching.\n\nLazyDictWithCache is useful for caching expensive computations, and\nfor implementing a lazy map in a single pass.  It is similar to\nthe Python dictionary, but has some additional features:\n* The dictionary uses an in-place hash table, which allows\n  efficient lookup and insertion.  This allows the dictionary\n  to be used as a key-value store without the overhead of\n  creating a new hash table each time.\n* The dictionary is immutable, so it can be used to implement\n  a lazy map or a lazy set, or to implement a lazy queue.\n* The dictionary is thread-safe, so it can be used to implement\n  a lazy map in a multi-threaded application.\n* The dictionary supports lazy set operations.  The dictionary\n  returns a new dictionary that contains the results of all\n  the set operations"
  },
  {
    "reference": "A dag (directed acyclic graph) is a collection of tasks with directional dependencies.\n\n    A dag also has a schedule, a start date and an end date (optional).  For each schedule,\n    (say daily or hourly), the DAG needs to run each individual tasks as their dependencies\n    are met. Certain tasks have the property of depending on their own past, meaning that\n    they can't run until their previous schedule (and upstream tasks) are completed.\n\n    DAGs essentially act as namespaces for tasks. A task_id can only be\n    added once to a DAG.\n\n    Note that if you plan to use time zones all the dates provided should be pendulum\n    dates. See :ref:`timezone_aware_dags`.\n\n    .. versionadded:: 2.4\n        The *schedule* argument to specify either time-based scheduling logic\n        (timetable), or asset-driven triggers.\n\n    .. versionchanged:: 3.0\n        The default value of *schedule* has been changed to *None* (no schedule).\n        The previous default was ``timedelta(days=1)``.\n\n    :param dag_id: The id of the DAG; must consist exclusively of alphanumeric\n        characters, dashes, dots and underscores (all ASCII)\n    :param description: The description for the DAG to e.g. be shown on the webserver\n    :param schedule: If provided, this defines the rules according to which DAG\n        runs are scheduled. Possible values include a cron expression string,\n        timedelta object, Timetable, or list of Asset objects.\n        See also :doc:`/howto/timetable`.\n    :param start_date: The timestamp from which the scheduler will\n        attempt to backfill. If this is not provided, backfilling must be done\n        manually with an explicit time range.\n    :param end_date: A date beyond which your DAG won't run, leave to None\n        for open-ended scheduling.\n    :param template_searchpath: This list of folders (non-relative)\n        defines where jinja will look for your templates. Order matters.\n        Note that jinja/airflow includes the path of your DAG file by\n        default\n    :param template_undefined: Template undefined type.\n    :param user_defined_macros: a dictionary of macros that will be exposed\n        in your jinja templates. For example, passing ``dict(foo='bar')``\n        to this argument allows you to ``{{ foo }}`` in all jinja\n        templates related to this DAG. Note that you can pass any\n        type of object here.\n    :param user_defined_filters: a dictionary of filters that will be exposed\n        in your jinja templates. For example, passing\n        ``dict(hello=lambda name: 'Hello %s' % name)`` to this argument allows\n        you to ``{{ 'world' | hello }}`` in all jinja templates related to\n        this DAG.\n    :param default_args: A dictionary of default parameters to be used\n        as constructor keyword parameters when initialising operators.\n        Note that operators have the same hook, and precede those defined\n        here, meaning that if your dict contains `'depends_on_past': True`\n        here and `'depends_on_past': False` in the operator's call\n        `default_args`, the actual value will be `False`.\n    :param params: a dictionary of DAG level parameters that are made\n        accessible in templates, namespaced under `params`. These\n        params can be overridden at the task level.\n    :param max_active_tasks: the number of task instances allowed to run\n        concurrently\n    :param max_active_runs: maximum number of active DAG runs, beyond this\n        number of DAG runs in a running state, the scheduler won't create\n        new active DAG runs\n    :param max_consecutive_failed_dag_runs: (experimental) maximum number of consecutive failed DAG runs,\n        beyond this the scheduler will disable the DAG\n    :param dagrun_timeout: Specify the duration a DagRun should be allowed to run before it times out or\n        fails. Task instances that are running when a DagRun is timed out will be marked as skipped.\n    :param sla_miss_callback: DEPRECATED - The SLA feature is removed in Airflow 3.0, to be replaced with a new implementation in 3.1\n    :param catchup: Perform scheduler catchup (or only run latest)? Defaults to False\n    :param on_failure_callback: A function or list of functions to be called when a DagRun of this dag fails.\n        A context dictionary is passed as a single parameter to this function.\n    :param on_success_callback: Much like the ``on_failure_callback`` except\n        that it is executed when the dag succeeds.\n    :param access_control: Specify optional DAG-level actions, e.g.,\n        \"{'role1': {'can_read'}, 'role2': {'can_read', 'can_edit', 'can_delete'}}\"\n        or it can specify the resource name if there is a DAGs Run resource, e.g.,\n        \"{'role1': {'DAG Runs': {'can_create'}}, 'role2': {'DAGs': {'can_read', 'can_edit', 'can_delete'}}\"\n    :param is_paused_upon_creation: Specifies if the dag is paused when created for the first time.\n        If the dag exists already, this flag will be ignored. If this optional parameter\n        is not specified, the global config setting will be used.\n    :param jinja_environment_kwargs: additional configuration options to be passed to Jinja\n        ``Environment`` for template rendering\n\n        **Example**: to avoid Jinja from removing a trailing newline from template strings ::\n\n            DAG(\n                dag_id=\"my-dag\",\n                jinja_environment_kwargs={\n                    \"keep_trailing_newline\": True,\n                    # some other jinja2 Environment options here\n                },\n            )\n\n        **See**: `Jinja Environment documentation\n        <https://jinja.palletsprojects.com/en/2.11.x/api/#jinja2.Environment>`_\n\n    :param render_template_as_native_obj: If True, uses a Jinja ``NativeEnvironment``\n        to render templates as native Python types. If False, a Jinja\n        ``Environment`` is used to render templates as string values.\n    :param tags: List of tags to help filtering DAGs in the UI.\n    :param owner_links: Dict of owners and their links, that will be clickable on the DAGs view UI.\n        Can be used as an HTTP link (for example the link to your Slack channel), or a mailto link.\n        e.g: {\"dag_owner\": \"https://airflow.apache.org/\"}\n    :param auto_register: Automatically register this DAG when it is used in a ``with`` block\n    :param fail_fast: Fails currently running tasks when task in DAG fails.\n        **Warning**: A fail fast dag can only have tasks with the default trigger rule (\"all_success\").\n        An exception will be thrown if any task in a fail fast dag has a non default trigger rule.\n    :param dag_display_name: The display name of the DAG which appears on the UI.",
    "prediction": "The handle_callback method will trigger on_success_callback or on_failure_callback, depending on the value of success, and return the context of the callback. This method is used by the Airflow scheduler to process a DAG run.\n\nThe on_failure_callback or on_success_callback is triggered if the dagrun is not complete. The context of the callback is returned by this method.\n\nIf the task has a failed state, then the callback will be called with a TaskInstance object that has the failed state. If the task has a completed state, then the callback will be called with a TaskInstance object that has the completed state. If the task has a failed or completed state, then the callback will be called with a TaskInstance object that has the failed or completed state, respectively. The reason will be the same as the reason that was passed to the on_success_callback or on_failure_callback method. This method is used by the Airflow scheduler to process a DAG run.\n\nIf the task is removed from the DAG, then the callback will be called with a TaskInstance object that has the removed state. The reason will be the same as the reason that was passed to the on_success_callback or on_failure_callback method. This method is used by the Airflow scheduler to process a DAG run."
  },
  {
    "reference": "States that a Task Instance can be in that indicate it is not yet in a terminal or running state.",
    "prediction": "Intermediate state for a task to be executed on the intermediate layer\n\"\"\"\n\nclass IntermediateTIStateInfo(str, Enum):\n    \n    INFO = \"info\"\n    WARNING = \"warning\"\n    ERROR = \"error\"\n    CANCELLED = \"cancelled\"\n\n    def __str__(self) -> str:\n        return self.value"
  },
  {
    "reference": "Exception raised when a model populates data interval fields incorrectly.\n\n    The data interval fields should either both be None (for runs scheduled\n    prior to AIP-39), or both be datetime (for runs scheduled after AIP-39 is\n    implemented). This is raised if exactly one of the fields is None.",
    "prediction": "Exception raised when a row has an inconsistent start and end field.\n\"\"\""
  },
  {
    "reference": "Raise when the task should be re-scheduled at a later time.\n\n    :param reschedule_date: The date when the task should be rescheduled",
    "prediction": "This class is used to serialize and deserialize AirflowRescheduleException.\n\"\"\""
  },
  {
    "reference": "Manages all provider distributions.\n\n    This is a Singleton class. The first time it is\n    instantiated, it discovers all available providers in installed packages.",
    "prediction": "Initialize the Airflow providers information and hooks.\n\"\"\"\n\n    def _discover_extra_links(self):\n        \"\"\"Retrieve extra_links information from the providers.\"\"\"\n        self._provider_dict = dict(sorted(self._provider_dict.items()))\n        extra_links = self._provider_dict.get(\"extra-links\", [])\n        for extra_link in extra_links:\n            self._discover_extra_link(extra_link)\n\n    def _discover_extra_link(self, extra_link):\n        \"\"\"Retrieve extra_link information from the providers.\"\"\"\n        # Retrieve the extra_link information from the provider\n        provider_name = extra_link[\"provider\"]\n        extra_link_type = extra_link[\"type\"]\n        extra_link_data = extra_link[\"data\"]\n        extra_link_provider_info = self._provider_dict.get(provider_name)\n        if extra_link_provider_info is None:\n            log.warning(\n                \"The extra-link provider '%s' could not be registered because provider '%s' \"\n                \"was not found.\",\n                extra_link_type,\n                provider_name,\n            )\n            return\n        extra_link_provider_info = self._provider_dict[provider_name]\n        extra_link_provider_info._extra_link = extra_link_provider_info._extra_link.copy()\n        extra_link_provider_info._extra_link.update(extra_link_data)\n        extra_link_provider_info._extra_link[\"provider\"]"
  },
  {
    "reference": "Table defining different owner attributes.\n\n    For example, a link for an owner that will be passed as a hyperlink to the \"DAGs\" view.",
    "prediction": "DAG_OWNER_ATTRIBUTES\n    Table for storing information about the DAG owner.\n\"\"\"\n\nclass DagOwner(Base):\n    __tablename__ = \"dag_owner\"\n    dag_id = Column(\n        StringID(),\n        ForeignKey(\"dag.dag_id\", name=\"dag.dag_id\", ondelete=\"CASCADE\"),\n        nullable=False,\n        primary_key=True,\n    )\n    owner = Column(String(500), nullable=False)\n    link = Column(String(500), nullable=False)\n\n    def __repr__(self):\n        return f\"<DagOwner: dag_id={self.dag_id}, owner={self.owner}>\"\n\n    @classmethod\n    def get_all(cls, session) -> dict[str, dict[str, str]]:\n        dag_links: dict = defaultdict(dict)\n        for obj in session.scalars(select(cls)):\n            dag_links[obj.dag_id].update({obj.owner: obj.link})\n        return dag_links"
  },
  {
    "reference": "Store info needed to create XComArg.\n\n    We can't turn it in to a XComArg until we've loaded _all_ the tasks, so when\n    deserializing an operator, we need to create something in its place, and\n    post-process it in ``deserialize_dag``.",
    "prediction": "This class provides a convenient way to store xcom data in a\nDAG.\n\"\"\""
  },
  {
    "reference": "Holds modifications to be applied when writing out the config.\n\n    :param rename: Mapping from (old_section, old_option) to (new_section, new_option)\n    :param remove: Set of (section, option) to remove\n    :param default_updates: Mapping from (section, option) to new default value",
    "prediction": "This class is a configuration class that allows to add new sections to the configuration,\nadd new options to an existing section, or to change the default value of an existing option.\n\"\"\""
  },
  {
    "reference": "Timetable that triggers DAG runs according to a cron expression.\n\n    This is different from ``CronDataIntervalTimetable``, where the cron\n    expression specifies the *data interval* of a DAG run. With this timetable,\n    the data intervals are specified independently from the cron expression.\n    Also for the same reason, this timetable kicks off a DAG run immediately at\n    the start of the period (similar to POSIX cron), instead of needing to wait\n    for one data interval to pass.\n\n    Don't pass ``@once`` in here; use ``OnceTimetable`` instead.\n\n    :param cron: cron string that defines when to run\n    :param timezone: Which timezone to use to interpret the cron string\n    :param interval: timedelta that defines the data interval start. Default 0.\n\n    *run_immediately* controls, if no *start_time* is given to the DAG, when\n    the first run of the DAG should be scheduled. It has no effect if there\n    already exist runs for this DAG.\n\n    * If *True*, always run immediately the most recent possible DAG run.\n    * If *False*, wait to run until the next scheduled time in the future.\n    * If passed a ``timedelta``, will run the most recent possible DAG run\n      if that run's ``data_interval_end`` is within timedelta of now.\n    * If *None*, the timedelta is calculated as 10% of the time between the\n      most recent past scheduled time and the next scheduled time. E.g. if\n      running every hour, this would run the previous time if less than 6\n      minutes had past since the previous run time, otherwise it would wait\n      until the next hour.",
    "prediction": "class CronTriggerTimetable(CronMixin, _TriggerTimetable):\n    def __init__(\n        self,\n        cron: str,\n        *,\n        timezone: str | Timezone | FixedTimezone,\n        interval: datetime.timedelta | relativedelta = datetime.timedelta(),\n        run_immediately: bool | datetime.timedelta = False,\n    ) -> None:\n        super().__init__(cron, timezone)\n        self._interval = interval\n        self._run_immediately = run_immediately\n\n    @classmethod\n    def deserialize(cls, data: dict[str, Any]) -> Timetable:\n        from airflow.serialization.serialized_objects import decode_timezone\n\n        return cls(\n            data[\"expression\"],\n            timezone=decode_timezone(data[\"timezone\"]),\n            interval=_deserialize_interval(data[\"interval\"]),\n            run_immediately=_deserialize_run_immediately(data.get(\"run_immediately\", False)),\n        )\n\n    def serialize(self) -> dict[str, Any]:\n        from airflow.serialization.serialized_objects import encode_timezone\n\n        return {\n            \"expression\": self._expression,\n            \"timezone\": encode_timezone(self._timezone),\n            \"interval\": _serialize_interval(self._interval),\n            \"run_immediately\": _serialize_run_immediately(self._run_immediately),\n        }\n\n    def _calc_first_run(self"
  },
  {
    "reference": "Collection of active assets.\n\n    An asset is considered active if it is declared by the user in any DAG files.\n    AssetModel entries that are not active (also called orphaned in some parts\n    of the code base) are still kept in the database, but have their corresponding\n    entries in this table removed. This ensures we keep all possible history on\n    distinct assets (those with non-matching name-URI pairs), but still ensure\n    *name and URI are each unique* within active assets.",
    "prediction": "This is the model class for the asset class.\n    Attributes:\n        name (string): Asset name\n        uri (string): Asset uri\n    \"\"\""
  },
  {
    "reference": "A base class for dependency contexts.\n\n    Specifies which dependencies should be evaluated in the context for a task\n    instance to satisfy the requirements of the context. Also stores state\n    related to the context that can be used by dependency classes.\n\n    For example there could be a SomeRunContext that subclasses this class which has\n    dependencies for:\n\n    - Making sure there are slots available on the infrastructure to run the task instance\n    - A task-instance's task-specific dependencies are met (e.g. the previous task\n      instance completed successfully)\n    - ...\n\n    :param deps: The context-specific dependencies that need to be evaluated for a\n        task instance to run in this execution context.\n    :param flag_upstream_failed: This is a hack to generate the upstream_failed state\n        creation while checking to see whether the task instance is runnable. It was the\n        shortest path to add the feature. This is bad since this class should be pure (no\n        side effects).\n    :param ignore_all_deps: Whether or not the context should ignore all ignorable\n        dependencies. Overrides the other ignore_* parameters\n    :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs (e.g. for\n        Backfills)\n    :param wait_for_past_depends_before_skipping: Wait for past depends before marking the ti as skipped\n    :param ignore_in_retry_period: Ignore the retry period for task instances\n    :param ignore_in_reschedule_period: Ignore the reschedule period for task instances\n    :param ignore_unmapped_tasks: Ignore errors about mapped tasks not yet being expanded\n    :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past and\n        trigger rule\n    :param ignore_ti_state: Ignore the task instance's previous failure/success\n    :param finished_tis: A list of all the finished task instances of this run",
    "prediction": "class DepContext:\n        deps: set = attr.ib(factory=set)\n        flag_upstream_failed: bool = False\n        ignore_all_deps: bool = False\n        ignore_depends_on_past: bool = False\n        wait_for_past_depends_before_skipping: bool = False\n        ignore_in_retry_period: bool = False\n        ignore_in_reschedule_period: bool = False\n        ignore_task_deps: bool = False\n        ignore_ti_state: bool = False\n        ignore_unmapped_tasks: bool = False\n        finished_tis: list[TaskInstance] | None = None\n        description: str | None = None\n        have_changed_ti_states: bool = False\n        \"\"\"Have any of the TIs state's been changed as a result of evaluating dependencies\"\"\"\n    \"\"\"\n    class DepContext:\n        deps: set = attr.ib(factory=set)\n        flag_upstream_failed: bool = False\n        ignore_all_deps: bool = False\n        ignore_depends_on_past: bool = False\n        wait_for_past_depends_before_skipping: bool = False\n        ignore_in_retry_period: bool = False\n        ignore_in_reschedule_period: bool = False\n        ignore_task_deps: bool = False\n        ignore_ti_state"
  },
  {
    "reference": "Request for DAG File Parsing.\n\n    This is the request that the manager will send to the DAG parser with the dag file and\n    any other necessary metadata.",
    "prediction": "The request for a dag file to parse. \n\"\"\""
  },
  {
    "reference": "Serializer for responses to bulk entity operations.\n\n    This represents the results of create, update, and delete actions performed on entity in bulk.\n    Each action (if requested) is represented as a field containing details about successful keys and any encountered errors.\n    Fields are populated in the response only if the respective action was part of the request, else are set None.",
    "prediction": "BulkActions are used to process many items in a single API call.\n    They are used for batch operations such as bulk delete, bulk create, and bulk update.\n    The bulk action returns the result of the operation and the status of the operation.\n    The result contains a `status` field which is either `OK` or `ERROR`.\n    If the operation was successful, the result also includes a `keys` field which is a list of keys that were created or updated.\n    If the operation failed, the result includes an `errors` field which is a list of error messages that occurred during the operation.\n    For example, if the operation failed to create a new key, the result will include an `errors` field with an error message like `\"Key already exists.\"`.\n    If the operation was successful, but there was a problem with the data that was being processed, the result will include an `errors` field with an error message like `\"Invalid input data.\"`.\n\n    The BulkActionResponse class contains the details of the bulk action, including the status of the operation and the keys that were created or updated.\n    The keys field contains a list of keys that were created or updated during the operation.\n    The keys field is not present if the operation"
  },
  {
    "reference": "TriggerRunnerSupervisor is responsible for monitoring the subprocess and marshalling DB access.\n\n    This class (which runs in the main process) is responsible for querying the DB, sending RunTrigger\n    workload messages to the subprocess, and collecting results and updating them in the DB.",
    "prediction": "A TriggerRunner is a class that can be started by `airflow.cli.trigger_runner.start` to start a subprocess\nthat will handle the job of running a trigger.\n\"\"\"\nclass TriggerRunner:\n    \"\"\"Run a trigger.\"\"\"\n\n    def __init__(self, job: Job):\n        \"\"\"Create a new TriggerRunner instance.\n\n        Args:\n            job: A Job object\n        \"\"\"\n        self._job = job\n\n    def start(self, id: int, target: str, **kwargs) -> None:\n        \"\"\"Start a TriggerRunner process.\n\n        Args:\n            id: A unique id for the subprocess. The subprocess will be started with this id.\n            target: The target that the subprocess will run in.\n            kwargs: Any extra keyword arguments will be passed to the subprocess.\n        \"\"\"\n        import subprocess\n        from airflow.sdk.execution_time.secrets_masker import reset_secrets_masker\n\n        reset_secrets_masker()\n        # Run the subprocess\n        proc = subprocess.Popen(\n            target=target,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            shell=False,\n            env={\"AIRFLOW_HOME\": self._job.get_airflow_home()},\n            cwd=self._job.get_airflow_home(),\n            close_fds=True,\n        )"
  },
  {
    "reference": "An implementation of Stats.Timer() which records the result in the OTel Metrics Map.\n\n    OpenTelemetry does not have a native timer, we will store the values as a Gauge.\n\n    :param name: The name of the timer.\n    :param tags: Tags to append to the timer.",
    "prediction": "Class for logging otel events in a custom format.\n\"\"\""
  },
  {
    "reference": "Ensures that the task instance's state is in a given set of valid states.\n\n    :param valid_states: A list of valid states that a task instance can have to meet\n        this dependency.\n    :return: whether or not the task instance's state is valid",
    "prediction": "ValidStateDep is used to set a task instance dependency's status to \"passing\" or \"failing\" when the task instance is in the specified valid states.\n\"\"\""
  },
  {
    "reference": "The resources required by an operator.\n\n    Resources that are not specified will use the default values from the airflow config.\n\n    :param cpus: The number of cpu cores that are required\n    :param ram: The amount of RAM required\n    :param disk: The amount of disk space required\n    :param gpus: The number of gpu units that are required",
    "prediction": "CpuResource - Resource class for cpu\n    RamResource - Resource class for ram\n    DiskResource - Resource class for disk\n    GpuResource - Resource class for gpus\n    Operators - Class for operators\n\"\"\""
  },
  {
    "reference": "Timetable that schedules data intervals with a cron expression.\n\n    This corresponds to ``schedule=<cron>``, where ``<cron>`` is either\n    a five/six-segment representation, or one of ``cron_presets``.\n\n    The implementation extends on croniter to add timezone awareness. This is\n    because croniter works only with naive timestamps, and cannot consider DST\n    when determining the next/previous time.\n\n    Don't pass ``@once`` in here; use ``OnceTimetable`` instead.",
    "prediction": "This class is for data interval timetables that are only triggered manually,\ne.g. when a DAG runs at a specific time.\n\"\"\"\n\nfrom datetime import timedelta, datetime\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\nfrom airflow.utils.dates import days_ago\nfrom airflow.operators.python import get_context\n\n\nclass CronDataIntervalTimetable(BaseOperator):\n    \"\"\"\n    A data interval timetable that is only triggered manually, e.g. when a DAG runs at a specific time.\n    \"\"\"\n\n    @apply_defaults\n    def __init__(\n        self,\n        expression: str,\n        timezone: str,\n        interval: timedelta,\n        **kwargs,\n    ) -> None:\n        super().__init__(**kwargs)\n        self._expression = expression\n        self._timezone = timezone\n        self._interval = timedelta(**interval)\n\n    def get_dagrun_kwargs(self) -> dict[str, Any]:\n        return {\"start_date\": days_ago(self._interval.days)}\n\n    def get_dagrun_timezone(self) -> str:\n        return self._timezone\n\n    def get_interval(self) -> timedelta:\n        return self._interval\n\n    def get_start_date(self) -> datetime:\n        return coerce_datetime(datetime.combine(days_ago(self._"
  },
  {
    "reference": "Base Trigger class.\n\n    Triggers are a workload that run in an asynchronous event loop shared with\n    other Triggers, and fire off events that will unpause deferred Tasks,\n    start linked DAGs, etc.\n\n    They are persisted into the database and then re-hydrated into a\n    \"triggerer\" process, where many are run at once. We model it so that\n    there is a many-to-one relationship between Task and Trigger, for future\n    deduplication logic to use.\n\n    Rows will be evicted from the database when the triggerer detects no\n    active Tasks/DAGs using them. Events are not stored in the database;\n    when an Event is fired, the triggerer will directly push its data to the\n    appropriate Task/DAG.",
    "prediction": "Trigger is a class that represents a trigger that is fired when a given condition is met.\n    It has a unique id, the classpath of the trigger, the encrypted kwargs of the trigger, and the\n    date the trigger was fired.\n    Trigger is an object that has a number of methods to create, rotate, and update a trigger.\n    Trigger can be created and assigned to a job.\n    Trigger can be assigned to an asset, which will trigger an event if the trigger is met.\n    Triggers can also be retrieved by ID and a trigger's ID can be used to retrieve a Trigger object.\n    Triggers can be updated, deleted, and rotated.\n    Trigger can be used to fire events to other assets.\n\"\"\""
  },
  {
    "reference": "Lazily build information from the serialized DAG structure.\n\n    An object that will present \"enough\" of the DAG like interface to update DAG db models etc, without having\n    to deserialize the full DAG and Task hierarchy.",
    "prediction": "This is a deserialized DAG model. This is a model that can be serialized and deserialized.\n\"\"\"\nfrom airflow.models.dag import DAG\nfrom airflow.models import DagRun\nfrom airflow.utils import timestamp\nfrom airflow.models import Variable\nfrom airflow.utils import timezone\nfrom airflow.utils import task_run_context\nfrom airflow.utils import dag\nfrom airflow.utils import timezone\nfrom airflow.utils import context\nfrom airflow.utils import dag as dag_utils\nfrom airflow.utils import task_run\nfrom airflow.utils import task\nfrom airflow.utils import dag_run\nfrom airflow.utils import dag_run as dag_run_utils\nfrom airflow.utils import task_run as task_run_utils\nfrom airflow.utils import task_run_utils as task_run_utils_utils\nfrom airflow.utils import task_run_utils as task_run_utils_utils\nfrom airflow.utils import dag_run_utils as dag_run_utils_utils\nfrom airflow.utils import dag_run_utils as dag_run_utils_utils\nfrom airflow.utils import task_run_utils as task_run_utils_utils\nfrom airflow.utils import dag_run_utils as dag_run_utils_utils\nfrom airflow.utils import task_run_utils as task_run_utils_utils\nfrom airflow.utils import dag_run_utils as dag_run_utils_utils\nfrom airflow.utils import task_run_utils as task_run_utils_utils\nfrom airflow.utils import dag_run_utils as"
  },
  {
    "reference": "An extension of FileHandler, advises the Kernel to not cache the file in PageCache when it is written.\n\n    While there is nothing wrong with such cache (it will be cleaned when memory is needed), it\n    causes ever-growing memory usage when scheduler is running as it keeps on writing new log\n    files and the files are not rotated later on. This might lead to confusion for our users,\n    who are monitoring memory usage of Scheduler - without realising that it is harmless and\n    expected in this case.\n\n    See https://github.com/apache/airflow/issues/14924\n\n    Adding the advice to Kernel might help with not generating the cache memory growth in the first place.",
    "prediction": "This class is a wrapper around a file handle that does not cache its\nstate, which makes it ideal for caching in Python. It is used by\nnon-caching file handlers.\n\"\"\""
  },
  {
    "reference": "SchedulerJobRunner runs for a specific time interval and schedules jobs that are ready to run.\n\n    It figures out the latest runs for each task and sees if the dependencies\n    for the next schedules are met.\n    If so, it creates appropriate TaskInstances and sends run commands to the\n    executor. It does this for each task in each DAG and repeats.\n\n    :param num_runs: The number of times to run the scheduling loop. If you\n        have a large number of DAG files this could complete before each file\n        has been parsed. -1 for unlimited times.\n    :param scheduler_idle_sleep_time: The number of seconds to wait between\n        polls of running processors\n    :param log: override the default Logger",
    "prediction": "This class implements the SchedulerJobRunner. It is responsible for\nexecuting tasks from a DAG run's scheduled tasks.\n\"\"\"\n\n    def _try_to_load_executor(self, executor_name: str) -> Executor:\n        \"\"\"Try to load an executor from the executor_loader.\"\"\"\n        from airflow.executors.base_executor import BaseExecutor\n\n        if not executor_name:\n            # If the executor name is None, we're not actually trying to load anything.\n            return None\n\n        try:\n            return self.job.executors[executor_name]\n        except KeyError:\n            self.log.warning(\n                \"Attempting to load executor '%s' which was not found in the executor_loader\",\n                executor_name,\n            )\n            return BaseExecutor()\n\n    def _try_to_load_executor_from_pool(self, executor_name: str) -> Executor:\n        \"\"\"Try to load an executor from the pool.\"\"\"\n        from airflow.executors.base_executor import BaseExecutor\n\n        try:\n            executor = self.job.executors[executor_name]\n        except KeyError:\n            self.log.warning(\n                \"Attempting to load executor '%s' which was not found in the executor_loader\",\n                executor_name,\n            )\n            return BaseExecutor()\n\n        return executor\n\n    def _try_to_load_executor_from_dag(self, dag_id:"
  },
  {
    "reference": "Handle all tracing requirements such as getting the tracer, and starting a new span.\n\n    When OTEL is enabled, the Trace class will be replaced by this class.",
    "prediction": "This class is a TracerProvider that will use a BatchSpanProcessor for producing spans.\nThis is useful when you want to instrument a task, but you don't want to have a batched span\nprocessor exporting spans to a collector at an interval.\n\nThe TracerProvider will use a SimpleSpanProcessor for the spans if it is used in a task, and a BatchSpanProcessor\nfor spans created from the task's code.\n\"\"\"\n\n\nclass BatchSpanProcessor(SpanProcessor):\n    def __init__(self, span_exporter: OTLPSpanExporter):\n        self.span_exporter = span_exporter\n\n    def _process_span(self, span: Span) -> Span:\n        if self.span_exporter:\n            # If span exporter is set, then it will export the span at the end of the task.\n            return self.span_exporter.export_span(span)\n        else:\n            # If span exporter is not set, then a SimpleSpanProcessor will be used.\n            return self.simple_processor.process_span(span)\n\n    def process_span(self, span: Span) -> Span:\n        return self._process_span(span)\n\n\nclass SimpleSpanProcessor(SpanProcessor):\n    def __init__(self, span_exporter: OTLPSpanExporter):\n        self.span_exporter"
  }
]