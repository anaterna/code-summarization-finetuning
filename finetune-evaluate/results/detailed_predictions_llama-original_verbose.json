[
  {
    "reference": "A table for serialized DAGs.\n\n    serialized_dag table is a snapshot of DAG files synchronized by scheduler.\n    This feature is controlled by:\n\n    * ``[core] min_serialized_dag_update_interval = 30`` (s):\n      serialized DAGs are updated in DB when a file gets processed by scheduler,\n      to reduce DB write rate, there is a minimal interval of updating serialized DAGs.\n    * ``[dag_processor] refresh_interval = 300`` (s):\n      interval of deleting serialized DAGs in DB when the files are deleted, suggest\n      to use a smaller interval such as 60\n    * ``[core] compress_serialized_dags``:\n      whether compressing the dag data to the Database.\n\n    It is used by webserver to load dags\n    because reading from database is lightweight compared to importing from files,\n    it solves the webserver scalability issue.",
    "prediction": "The SerializedDagModel class is used to serialize DAG objects to the serialized_dag table\nand later to read them back.\n\"\"\"\n\n# (truncated)..."
  },
  {
    "reference": "Lazy-loaded cached dictionary.\n\n    Dictionary, which in case you set callable, executes the passed callable with `key` attribute\n    at first use - and returns and caches the result.",
    "prediction": "LazyDictWithCache is a lazy dictionary with a cache. The cache is a dict\n    where the key is a (key, value) pair and the value is a callable that\n    will return the value when called. The key can be any type.\n    When the cache is first constructed, it will be populated with all\n    the items in the dictionary. Subsequent calls to get the value for a\n    key will return the cached value. If the key is not in the cache, then\n    the callable will be called and the value returned.\n\n    This is similar to the DictWithCache class from the standard library,\n    but the caching is done lazily. This makes the DictWithCache class\n    a bit faster than the standard library one, but also means that you can\n    never have a reference to the dictionary that is not in the cache, so\n    it's not a good idea to use the cache for large dictionaries.\n    \"\"\""
  },
  {
    "reference": "A dag (directed acyclic graph) is a collection of tasks with directional dependencies.\n\n    A dag also has a schedule, a start date and an end date (optional).  For each schedule,\n    (say daily or hourly), the DAG needs to run each individual tasks as their dependencies\n    are met. Certain tasks have the property of depending on their own past, meaning that\n    they can't run until their previous schedule (and upstream tasks) are completed.\n\n    DAGs essentially act as namespaces for tasks. A task_id can only be\n    added once to a DAG.\n\n    Note that if you plan to use time zones all the dates provided should be pendulum\n    dates. See :ref:`timezone_aware_dags`.\n\n    .. versionadded:: 2.4\n        The *schedule* argument to specify either time-based scheduling logic\n        (timetable), or asset-driven triggers.\n\n    .. versionchanged:: 3.0\n        The default value of *schedule* has been changed to *None* (no schedule).\n        The previous default was ``timedelta(days=1)``.\n\n    :param dag_id: The id of the DAG; must consist exclusively of alphanumeric\n        characters, dashes, dots and underscores (all ASCII)\n    :param description: The description for the DAG to e.g. be shown on the webserver\n    :param schedule: If provided, this defines the rules according to which DAG\n        runs are scheduled. Possible values include a cron expression string,\n        timedelta object, Timetable, or list of Asset objects.\n        See also :doc:`/howto/timetable`.\n    :param start_date: The timestamp from which the scheduler will\n        attempt to backfill. If this is not provided, backfilling must be done\n        manually with an explicit time range.\n    :param end_date: A date beyond which your DAG won't run, leave to None\n        for open-ended scheduling.\n    :param template_searchpath: This list of folders (non-relative)\n        defines where jinja will look for your templates. Order matters.\n        Note that jinja/airflow includes the path of your DAG file by\n        default\n    :param template_undefined: Template undefined type.\n    :param user_defined_macros: a dictionary of macros that will be exposed\n        in your jinja templates. For example, passing ``dict(foo='bar')``\n        to this argument allows you to ``{{ foo }}`` in all jinja\n        templates related to this DAG. Note that you can pass any\n        type of object here.\n    :param user_defined_filters: a dictionary of filters that will be exposed\n        in your jinja templates. For example, passing\n        ``dict(hello=lambda name: 'Hello %s' % name)`` to this argument allows\n        you to ``{{ 'world' | hello }}`` in all jinja templates related to\n        this DAG.\n    :param default_args: A dictionary of default parameters to be used\n        as constructor keyword parameters when initialising operators.\n        Note that operators have the same hook, and precede those defined\n        here, meaning that if your dict contains `'depends_on_past': True`\n        here and `'depends_on_past': False` in the operator's call\n        `default_args`, the actual value will be `False`.\n    :param params: a dictionary of DAG level parameters that are made\n        accessible in templates, namespaced under `params`. These\n        params can be overridden at the task level.\n    :param max_active_tasks: the number of task instances allowed to run\n        concurrently\n    :param max_active_runs: maximum number of active DAG runs, beyond this\n        number of DAG runs in a running state, the scheduler won't create\n        new active DAG runs\n    :param max_consecutive_failed_dag_runs: (experimental) maximum number of consecutive failed DAG runs,\n        beyond this the scheduler will disable the DAG\n    :param dagrun_timeout: Specify the duration a DagRun should be allowed to run before it times out or\n        fails. Task instances that are running when a DagRun is timed out will be marked as skipped.\n    :param sla_miss_callback: DEPRECATED - The SLA feature is removed in Airflow 3.0, to be replaced with a new implementation in 3.1\n    :param catchup: Perform scheduler catchup (or only run latest)? Defaults to False\n    :param on_failure_callback: A function or list of functions to be called when a DagRun of this dag fails.\n        A context dictionary is passed as a single parameter to this function.\n    :param on_success_callback: Much like the ``on_failure_callback`` except\n        that it is executed when the dag succeeds.\n    :param access_control: Specify optional DAG-level actions, e.g.,\n        \"{'role1': {'can_read'}, 'role2': {'can_read', 'can_edit', 'can_delete'}}\"\n        or it can specify the resource name if there is a DAGs Run resource, e.g.,\n        \"{'role1': {'DAG Runs': {'can_create'}}, 'role2': {'DAGs': {'can_read', 'can_edit', 'can_delete'}}\"\n    :param is_paused_upon_creation: Specifies if the dag is paused when created for the first time.\n        If the dag exists already, this flag will be ignored. If this optional parameter\n        is not specified, the global config setting will be used.\n    :param jinja_environment_kwargs: additional configuration options to be passed to Jinja\n        ``Environment`` for template rendering\n\n        **Example**: to avoid Jinja from removing a trailing newline from template strings ::\n\n            DAG(\n                dag_id=\"my-dag\",\n                jinja_environment_kwargs={\n                    \"keep_trailing_newline\": True,\n                    # some other jinja2 Environment options here\n                },\n            )\n\n        **See**: `Jinja Environment documentation\n        <https://jinja.palletsprojects.com/en/2.11.x/api/#jinja2.Environment>`_\n\n    :param render_template_as_native_obj: If True, uses a Jinja ``NativeEnvironment``\n        to render templates as native Python types. If False, a Jinja\n        ``Environment`` is used to render templates as string values.\n    :param tags: List of tags to help filtering DAGs in the UI.\n    :param owner_links: Dict of owners and their links, that will be clickable on the DAGs view UI.\n        Can be used as an HTTP link (for example the link to your Slack channel), or a mailto link.\n        e.g: {\"dag_owner\": \"https://airflow.apache.org/\"}\n    :param auto_register: Automatically register this DAG when it is used in a ``with`` block\n    :param fail_fast: Fails currently running tasks when task in DAG fails.\n        **Warning**: A fail fast dag can only have tasks with the default trigger rule (\"all_success\").\n        An exception will be thrown if any task in a fail fast dag has a non default trigger rule.\n    :param dag_display_name: The display name of the DAG which appears on the UI.",
    "prediction": "This is the method to handle callback events for a dag run. The callback\nevents are triggered when a DAG is run.\n\nFor example, if a DAG is run, the following are the events that will be\ntriggered:\n\n- on_success_callback: if the DAG is successful, the on_success_callback\n  is triggered\n- on_failure_callback: if the DAG is not successful, the on_failure_callback\n  is triggered\n- on_retry_callback: if the DAG is retried, the on_retry_callback is triggered\n\nThe following events are not triggered:\n\n- on_cancel_callback: if the DAG is cancelled, the on_cancel_callback is\n  not triggered\n- on_failure_callback: if the DAG is not successful, the on_failure_callback\n  is not triggered\n- on_retry_callback: if the DAG is retried, the on_retry_callback is not\n  triggered\n\"\"\"\n        if success:\n            # Trigger the on_success_callback\n            self.on_success_callback(dagrun, reason)\n        else:\n            # Trigger the on_failure_callback\n            self.on_failure_callback(dagrun, reason)\n        # Trigger the on_retry_callback\n        if dagrun.retry:\n            self.on_retry_callback(dagrun, reason)\n\n    @provide_session\n    def"
  },
  {
    "reference": "States that a Task Instance can be in that indicate it is not yet in a terminal or running state.",
    "prediction": "This class contains the states of a TI instance. A state is a transition from one state to another.\nA TI instance has multiple states, each of which represents a particular state in the system.\nFor example, a state might be \"scheduled\" or \"upstream_failed\". A state can have multiple transitions.\nFor example, a state can transition from \"scheduled\" to \"upstream_failed\" because the system has failed\nto schedule the job for some reason. A state can also transition from \"upstream_failed\" to \"upstream_failed\".\nA state can also transition from \"upstream_failed\" to \"scheduled\" if the system has been restarted and\nis now in the \"scheduled\" state.\n\nThe states are defined in the class IntermediateTIState.\n\"\"\"\n\nclass IntermediateTIInstance(object):\n    \"\"\"A state of a TI instance.\n    This class represents a state in the TI instance.\n    It has a name, which is a string representing the state.\n    It also has a list of transitions, which are strings representing the transitions.\n    \"\"\"\n\n    def __init__(self, name: str, transitions: List[str]) -> None:\n        self.name = name\n        self.transitions = transitions\n        self.current_state = None\n        self.next_state = None"
  },
  {
    "reference": "Exception raised when a model populates data interval fields incorrectly.\n\n    The data interval fields should either both be None (for runs scheduled\n    prior to AIP-39), or both be datetime (for runs scheduled after AIP-39 is\n    implemented). This is raised if exactly one of the fields is None.",
    "prediction": "\"\"\"\n# pylint: disable=invalid-name\n# pylint: disable=too-many-arguments\n\n# pylint: disable=too-many-branches\n# pylint: disable=too-many-locals\n# pylint: disable=too-many-statements\n# pylint: disable=too-many-words\n# pylint: disable=too-many-instance-attributes\n# pylint: disable=too-many-arguments\n# pylint: disable=too-many-arguments\n\n# pylint: disable=too-many-locals\n# pylint: disable=too-many-statements\n# pylint: disable=too-many-branches\n# pylint: disable=too-many-arguments\n\n# pylint: disable=too-many-words\n# pylint: disable=too-many-instance-attributes\n# pylint: disable=too-many-arguments\n# pylint: disable=too-many-arguments\n\n# pylint: disable=too-many-arguments\n# pylint: disable=too-many-locals\n# pylint: disable=too-many-branches\n# pylint: disable=too-many-words\n# pylint: disable=too-many-instance-attributes\n# pylint: disable=too-many-arguments\n\n# pylint: disable=too-many-arguments\n# pylint: disable=too-many-locals\n# pylint: disable"
  },
  {
    "reference": "Raise when the task should be re-scheduled at a later time.\n\n    :param reschedule_date: The date when the task should be rescheduled",
    "prediction": "The AirflowRescheduleException class is an exception that is raised when the\nairflow scheduler is rescheduled. It contains the date when the rescheduling\noccurred.\n\"\"\"\n\n\nclass AirflowException(AirflowException):\n    \"\"\"AirflowException is the base class for all exceptions that occur when\n    airflow is running.\n    \"\"\"\n\n    def __init__(self, msg=None):\n        super().__init__()\n        self.msg = msg\n\n    def serialize(self):\n        cls = self.__class__\n        return f\"{cls.__module__}.{cls.__name__}\", (), {\"msg\": self.msg}\n\n    def __str__(self):\n        return self.msg"
  },
  {
    "reference": "Manages all provider distributions.\n\n    This is a Singleton class. The first time it is\n    instantiated, it discovers all available providers in installed packages.",
    "prediction": "This class is used to manage providers in Airflow. It keeps a dict of providers keyed by\nthe package name. The dict contains ProviderInfo objects for each provider.\nThe ProviderInfo object contains version, connection-types, connection-testable and\npackage-info.\n\nThe PackageInfo object contains the package name, the path to the package folder and\nthe name of the package.\n\"\"\"\n\n    def _discover_all_providers_from_packages(self) -> None:\n        \"\"\"\n        Discover all providers by scanning packages installed.\n\n        The list of providers should be returned via the 'apache_airflow_provider'\n        entrypoint as a dictionary conforming to the 'airflow/provider_info.schema.json'\n        schema. Note that the schema is different at runtime than provider.yaml.schema.json.\n        The development version of provider schema is more strict and changes together with\n        the code. The runtime version is more relaxed (allows for additional properties)\n        and verifies only the subset of fields that are needed at runtime.\n        \"\"\"\n        for entry_point, dist in entry_points_with_dist(\"apache_airflow_provider\"):\n            package_name = canonicalize_name(dist.metadata[\"name\"])\n            if package_name in self._provider_dict:\n                continue\n            log.debug(\"Loading %s from package %s\", entry_point,"
  },
  {
    "reference": "Table defining different owner attributes.\n\n    For example, a link for an owner that will be passed as a hyperlink to the \"DAGs\" view.",
    "prediction": "class DagOwnerAttributes(Base):\n        __tablename__ = \"dag_owner_attributes\"\n        dag_id = Column(\n            StringID(),\n            ForeignKey(\"dag.dag_id\", name=\"dag.dag_id\", ondelete=\"CASCADE\"),\n            nullable=False,\n            primary_key=True,\n        )\n        owner = Column(String(500), primary_key=True, nullable=False)\n        link = Column(String(500), nullable=False)\n\n        def __repr__(self):\n            return f\"<DagOwnerAttributes: dag_id={self.dag_id}, owner={self.owner}, link={self.link}>\"\n\n        @classmethod\n        def get_all(cls, session) -> dict[str, dict[str, str]]:\n            dag_links: dict = defaultdict(dict)\n            for obj in session.scalars(select(cls)):\n                dag_links[obj.dag_id].update({obj.owner: obj.link})\n            return dag_links\n\"\"\""
  },
  {
    "reference": "Store info needed to create XComArg.\n\n    We can't turn it in to a XComArg until we've loaded _all_ the tasks, so when\n    deserializing an operator, we need to create something in its place, and\n    post-process it in ``deserialize_dag``.",
    "prediction": "This is a named tuple to be used as a type hint in a function signature.\nIt is used for serialization and deserialization of XCom arguments.\n\nThe type hint for a function is a tuple of arguments, and the type hint for a\nfunction's return value is a tuple of values. The values are the types of the\nreturned values, and the types are the types of the arguments.\n\"\"\"\n\n\nclass _XComRef(NamedTuple):\n    data: dict\n\n    def deref(self, dag: DAG) -> SchedulerXComArg:\n        return deserialize_xcom_arg(self.data, dag)\n\n\ndef deserialize_xcom_arg(data: dict, dag: DAG) -> SchedulerXComArg:\n    \"\"\"\n    Deserialize a XCom argument from a serialized version of it.\n    \"\"\"\n    if not data:\n        return None\n\n    if isinstance(data, dict):\n        if data.get('type'):\n            if data.get('type') =='scheduler':\n                return SchedulerXComArg(dag, data['data'])\n        elif data.get('type') =='scheduler-arg':\n            return SchedulerXComArg(dag, data['data']['arg'], data['data']['value'])\n        elif data.get('type') =='scheduler-arg-arg':\n            return"
  },
  {
    "reference": "Holds modifications to be applied when writing out the config.\n\n    :param rename: Mapping from (old_section, old_option) to (new_section, new_option)\n    :param remove: Set of (section, option) to remove\n    :param default_updates: Mapping from (section, option) to new default value",
    "prediction": "This class holds the configuration modifications made to the configuration file.\n\"\"\""
  },
  {
    "reference": "Timetable that triggers DAG runs according to a cron expression.\n\n    This is different from ``CronDataIntervalTimetable``, where the cron\n    expression specifies the *data interval* of a DAG run. With this timetable,\n    the data intervals are specified independently from the cron expression.\n    Also for the same reason, this timetable kicks off a DAG run immediately at\n    the start of the period (similar to POSIX cron), instead of needing to wait\n    for one data interval to pass.\n\n    Don't pass ``@once`` in here; use ``OnceTimetable`` instead.\n\n    :param cron: cron string that defines when to run\n    :param timezone: Which timezone to use to interpret the cron string\n    :param interval: timedelta that defines the data interval start. Default 0.\n\n    *run_immediately* controls, if no *start_time* is given to the DAG, when\n    the first run of the DAG should be scheduled. It has no effect if there\n    already exist runs for this DAG.\n\n    * If *True*, always run immediately the most recent possible DAG run.\n    * If *False*, wait to run until the next scheduled time in the future.\n    * If passed a ``timedelta``, will run the most recent possible DAG run\n      if that run's ``data_interval_end`` is within timedelta of now.\n    * If *None*, the timedelta is calculated as 10% of the time between the\n      most recent past scheduled time and the next scheduled time. E.g. if\n      running every hour, this would run the previous time if less than 6\n      minutes had past since the previous run time, otherwise it would wait\n      until the next hour.",
    "prediction": "A class that implements a CronTriggerTimetable.\n\"\"\""
  },
  {
    "reference": "Collection of active assets.\n\n    An asset is considered active if it is declared by the user in any DAG files.\n    AssetModel entries that are not active (also called orphaned in some parts\n    of the code base) are still kept in the database, but have their corresponding\n    entries in this table removed. This ensures we keep all possible history on\n    distinct assets (those with non-matching name-URI pairs), but still ensure\n    *name and URI are each unique* within active assets.",
    "prediction": "An asset is a collection of related information, usually\n    in a single file, that is shared between multiple\n    clients.\n    Assets are the foundation of many of the other models\n    in this project, including the client and the\n    model-level relationships that are required to\n    represent the model's assets.\n    A single asset can be related to multiple clients, and\n    each client can have multiple assets.\n    Assets are used to represent the content of a single\n    file, such as a PDF, or a series of files, such as\n    an audio or video file.\n    Assets can be used to represent any kind of content, such\n    as a model or a document.\n    Assets can be represented in a variety of ways, such as\n    a file on a local computer, or a server that is\n    accessible via the web.\n    Assets can be used to represent any kind of content, such\n    as a model or a document.\n    Assets can be represented in a variety of ways, such as\n    a file on a local computer, or a server that is\n    accessible via the web.\n    Assets can be used to represent any kind of content, such\n    as a model or a document"
  },
  {
    "reference": "A base class for dependency contexts.\n\n    Specifies which dependencies should be evaluated in the context for a task\n    instance to satisfy the requirements of the context. Also stores state\n    related to the context that can be used by dependency classes.\n\n    For example there could be a SomeRunContext that subclasses this class which has\n    dependencies for:\n\n    - Making sure there are slots available on the infrastructure to run the task instance\n    - A task-instance's task-specific dependencies are met (e.g. the previous task\n      instance completed successfully)\n    - ...\n\n    :param deps: The context-specific dependencies that need to be evaluated for a\n        task instance to run in this execution context.\n    :param flag_upstream_failed: This is a hack to generate the upstream_failed state\n        creation while checking to see whether the task instance is runnable. It was the\n        shortest path to add the feature. This is bad since this class should be pure (no\n        side effects).\n    :param ignore_all_deps: Whether or not the context should ignore all ignorable\n        dependencies. Overrides the other ignore_* parameters\n    :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs (e.g. for\n        Backfills)\n    :param wait_for_past_depends_before_skipping: Wait for past depends before marking the ti as skipped\n    :param ignore_in_retry_period: Ignore the retry period for task instances\n    :param ignore_in_reschedule_period: Ignore the reschedule period for task instances\n    :param ignore_unmapped_tasks: Ignore errors about mapped tasks not yet being expanded\n    :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past and\n        trigger rule\n    :param ignore_ti_state: Ignore the task instance's previous failure/success\n    :param finished_tis: A list of all the finished task instances of this run",
    "prediction": "This class contains all the information needed to track dependencies and to track a task's state.\n    \"\"\"\n\n    def __init__(self, task: Task, session: Session, dag_run: DagRun):\n        self.task = task\n        self.session = session\n        self.dag_run = dag_run\n        self.task_id = task.task_id\n        self.task_name = task.name\n        self.task_id_str = task.task_id\n        self.dag = task.dag\n        self.dag_run_id = task.dag_run_id\n        self.task_dependencies = set()\n        self.task_dependencies.add(task)\n        self.dependencies = set()\n        self._ensure_dependencies()\n        self._ensure_finished_tasks()\n        self._ensure_started_tasks()\n        self._ensure_skipped_tasks()\n        self._ensure_tis_skipped()\n        self._ensure_finished_tis()\n\n    def _"
  },
  {
    "reference": "Request for DAG File Parsing.\n\n    This is the request that the manager will send to the DAG parser with the dag file and\n    any other necessary metadata.",
    "prediction": "This class is the entry point to the DagFileParseRequest.\nIt will parse the given file and call the callback functions. The callback functions are\npassed a `CallbackRequest` object as an argument. The `CallbackRequest` object contains\nthe following properties:\n    - file: The path to the file that was parsed.\n    - bundle_path: The path to the bundle that was parsed.\n    - requests_fd: The file descriptor of the file that was parsed.\n    - callback_requests: The list of callback functions that were called.\n\"\"\"\n\n\ndef parse(file: Path) -> \"DagFileParseRequest\":\n    \"\"\"Parse a file into a `DagFileParseRequest` object.\"\"\"\n    return DagFileParseRequest(\n        file=file,\n        bundle_path=file.parent,\n        requests_fd=-1,\n        callback_requests=list(),\n    )"
  },
  {
    "reference": "Serializer for responses to bulk entity operations.\n\n    This represents the results of create, update, and delete actions performed on entity in bulk.\n    Each action (if requested) is represented as a field containing details about successful keys and any encountered errors.\n    Fields are populated in the response only if the respective action was part of the request, else are set None.",
    "prediction": "A bulk action response, or more specifically, an error response.\n\"\"\""
  },
  {
    "reference": "TriggerRunnerSupervisor is responsible for monitoring the subprocess and marshalling DB access.\n\n    This class (which runs in the main process) is responsible for querying the DB, sending RunTrigger\n    workload messages to the subprocess, and collecting results and updating them in the DB.",
    "prediction": "\"\"\"\nfrom airflow import airflow\nfrom airflow.models import TriggerRunnerSupervisor\nfrom airflow.exceptions import AirflowException\nfrom airflow.operators.trigger import Trigger\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.providers.postgres.hooks.postgres import PostgresHookPostgres as PostgresHook\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom airflow.providers.postgres.operators.postgres import PostgresOperatorRunTrigger\nfrom airflow.providers.postgres.operators.postgres import PostgresOperatorRunTriggerWithTaskInstance\nfrom airflow.providers.postgres.hooks.postgres import PostgresHookPostgres\nfrom airflow.utils import context\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.utils.state import State\nfrom airflow.utils.task_group import TaskGroup\nfrom airflow.utils.state import State\nfrom airflow.utils.task import TaskGroup\n\nfrom airflow.providers.postgres.hooks.postgres import PostgresHookPostgres\n\nfrom airflow.exceptions import AirflowException\nfrom airflow.providers.postgres.hooks.postgres import PostgresHookPostgres\nfrom airflow.providers.postgres.hooks.postgres import PostgresHookPostgres\nfrom airflow.providers.postgres.hooks.postgres import PostgresHookPostgres\nfrom airflow.providers.postgres.hooks.post"
  },
  {
    "reference": "An implementation of Stats.Timer() which records the result in the OTel Metrics Map.\n\n    OpenTelemetry does not have a native timer, we will store the values as a Gauge.\n\n    :param name: The name of the timer.\n    :param tags: Tags to append to the timer.",
    "prediction": "This class represents the Timer class. It is used to measure the time taken by a method call. It can also be used to get the duration of a method call. It is a Singleton class.\n\"\"\""
  },
  {
    "reference": "Ensures that the task instance's state is in a given set of valid states.\n\n    :param valid_states: A list of valid states that a task instance can have to meet\n        this dependency.\n    :return: whether or not the task instance's state is valid",
    "prediction": "A task instance dependency that only requires the task instance state to be valid.\n\"\"\""
  },
  {
    "reference": "The resources required by an operator.\n\n    Resources that are not specified will use the default values from the airflow config.\n\n    :param cpus: The number of cpu cores that are required\n    :param ram: The amount of RAM required\n    :param disk: The amount of disk space required\n    :param gpus: The number of gpu units that are required",
    "prediction": "CpuResource represents a single CPU.\nRamResource represents a single RAM.\nDiskResource represents a single disk.\nGpuResource represents a single GPU.\n\"\"\"\n\n\nclass CpuResource:\n    \"\"\"Represents a single CPU.\"\"\"\n\n    def __init__(self, cpus: int):\n        self.cpus = cpus\n\n    def to_dict(self):\n        return {\n            \"qty\": self.cpus,\n        }\n\n    @classmethod\n    def from_dict(cls, cpus_dict: dict):\n        \"\"\"Create CpuResource from cpus dict.\"\"\"\n        return cls(cpus=cpus_dict[\"qty\"])\n\n\nclass RamResource:\n    \"\"\"Represents a single RAM.\"\"\"\n\n    def __init__(self, ram: int):\n        self.ram = ram\n\n    def to_dict(self):\n        return {\n            \"qty\": self.ram,\n        }\n\n    @classmethod\n    def from_dict(cls, ram_dict: dict):\n        \"\"\"Create RamResource from ram dict.\"\"\"\n        return cls(ram=ram_dict[\"qty\"])\n\n\nclass DiskResource:\n    \"\"\"Represents a single disk.\"\"\"\n\n    def __init__(self, disk: int):\n        self.disk = disk\n\n    def to_dict(self):\n        return {\n            \"qty\": self.disk,\n        }"
  },
  {
    "reference": "Timetable that schedules data intervals with a cron expression.\n\n    This corresponds to ``schedule=<cron>``, where ``<cron>`` is either\n    a five/six-segment representation, or one of ``cron_presets``.\n\n    The implementation extends on croniter to add timezone awareness. This is\n    because croniter works only with naive timestamps, and cannot consider DST\n    when determining the next/previous time.\n\n    Don't pass ``@once`` in here; use ``OnceTimetable`` instead.",
    "prediction": "This class is a wrapper around the DataIntervalTimetable, allowing to\n    create DataIntervalTimetable objects using Pythonic syntax.\n    It also provides a way to create a DataIntervalTimetable object using\n    data from an Airflow object.\n    \"\"\"\nclass DataIntervalTimetable(CronMixin, _DataIntervalTimetable):\n    \n    def __init__(self, *args, **kwargs):\n        from airflow.serialization.serialized_objects import decode_timezone\n\n        super().__init__(*args, **kwargs)\n        self._expression = self._expression\n        self._timezone = self._timezone\n\n    def serialize(self) -> dict[str, Any]:\n        from airflow.serialization.serialized_objects import encode_timezone\n\n        return {\"expression\": self._expression, \"timezone\": encode_timezone(self._timezone)}\n\n    def deserialize(self, data: dict[str, Any]) -> Timetable:\n        from airflow.serialization.serialized_objects import decode_timezone\n\n        return self.__class__(data[\"expression\"], decode_timezone(data[\"timezone\"]))\n\n    def infer_manual_data_interval(self, *, run_after: DateTime) -> DataInterval:\n        # Get the last complete period before run_after, e.g. if a DAG run is\n        # scheduled at each midnight, the data interval of a"
  },
  {
    "reference": "Base Trigger class.\n\n    Triggers are a workload that run in an asynchronous event loop shared with\n    other Triggers, and fire off events that will unpause deferred Tasks,\n    start linked DAGs, etc.\n\n    They are persisted into the database and then re-hydrated into a\n    \"triggerer\" process, where many are run at once. We model it so that\n    there is a many-to-one relationship between Task and Trigger, for future\n    deduplication logic to use.\n\n    Rows will be evicted from the database when the triggerer detects no\n    active Tasks/DAGs using them. Events are not stored in the database;\n    when an Event is fired, the triggerer will directly push its data to the\n    appropriate Task/DAG.",
    "prediction": "This is a trigger class that represents a Trigger object in the DB.\n\"\"\"\n\n\nclass TriggerFailureReason(str, enum.Enum):\n    TRIGGER_FAILURE = \"Trigger failure\""
  },
  {
    "reference": "Lazily build information from the serialized DAG structure.\n\n    An object that will present \"enough\" of the DAG like interface to update DAG db models etc, without having\n    to deserialize the full DAG and Task hierarchy.",
    "prediction": "This class implements the lazy serialization of dag objects. It is used for\nserialization of dag objects which are stored in the database but are not\nused immediately. For example, a dag is serialized in the database, and the\ndag object is not used until the dag run is created.\n\"\"\"\n\nclass SerializedDAG(pydantic.BaseModel):\n    \"\"\"\n    This class represents a serialized DAG.\n\n    :param serialized_dag: The serialized DAG object.\n    \"\"\"\n\n    dag_id: str\n    _cached_serialized_dag: SerializedDAG | None = None\n    data: dict\n    _cached_serialized_dag: SerializedDAG | None = None\n    dag_display_name: str\n    description: str\n    max_active_tasks: int\n    max_active_runs: int\n    max_consecutive_failed_dag_runs: int\n    owner: str\n    access_control: dict\n    dag: dict\n    _cached_serialized_dag: SerializedDAG | None = None\n    _cached_serialized_dag: SerializedDAG | None = None\n    _real_dag: SerializedDAG\n    _cached_serialized_dag: SerializedDAG | None = None\n    _cached_serialized_dag: Serialized"
  },
  {
    "reference": "An extension of FileHandler, advises the Kernel to not cache the file in PageCache when it is written.\n\n    While there is nothing wrong with such cache (it will be cleaned when memory is needed), it\n    causes ever-growing memory usage when scheduler is running as it keeps on writing new log\n    files and the files are not rotated later on. This might lead to confusion for our users,\n    who are monitoring memory usage of Scheduler - without realising that it is harmless and\n    expected in this case.\n\n    See https://github.com/apache/airflow/issues/14924\n\n    Adding the advice to Kernel might help with not generating the cache memory growth in the first place.",
    "prediction": "A file handler that does not cache file accesses.\n\nThe advantage of using this handler is that it does not require the\nfile to be cached. This is useful if the file is a cache of data\nthat is updated frequently, and the file is accessed often.\n\nIf a file is opened using this handler, the file will be opened\nfor the first time, and will be cached.  If the file is re-opened\nfor subsequent accesses, the cached version will be used.  The\nfile will not be re-opened for subsequent accesses, and the\nfile will not be cached.  Thus, if a file is opened using this\nhandler, the file will be opened for the first time, and the\nfile will not be cached.  If a file is re-opened for subsequent\naccesses, the cached version will be used.\n\"\"\"\n\nclass NonCachingFileHandler(FileHandler):\n    \n\n    def _open(self):\n        return make_file_io_non_caching(super()._open())"
  },
  {
    "reference": "SchedulerJobRunner runs for a specific time interval and schedules jobs that are ready to run.\n\n    It figures out the latest runs for each task and sees if the dependencies\n    for the next schedules are met.\n    If so, it creates appropriate TaskInstances and sends run commands to the\n    executor. It does this for each task in each DAG and repeats.\n\n    :param num_runs: The number of times to run the scheduling loop. If you\n        have a large number of DAG files this could complete before each file\n        has been parsed. -1 for unlimited times.\n    :param scheduler_idle_sleep_time: The number of seconds to wait between\n        polls of running processors\n    :param log: override the default Logger",
    "prediction": "The SchedulerJobRunner class provides a base class for scheduling jobs.\n\nA scheduler job is a job that has been registered in the scheduler with a DAG that has a\nDagRun object.\n\nA scheduler job runner is a subclass of SchedulerJobRunner that schedules a job. A scheduler job\nrunner is responsible for:\n- Finding TIs that are ready for execution based on conditions.\n- Loading the DAG and TIs.\n- Loading the DAG and TIs into the scheduler job.\n- Executing the job.\n- Saving the job's state.\n\"\"\"\n\n    def _try_to_load_executor(self, executor: Executor) -> Executor:\n        if not executor.name:\n            return None\n\n        try:\n            return self._executor_loader.load_executor(executor.name)\n        except ExecutorLoaderError:\n            return None\n\n    def _load_dag(self, dag_run: DagRun) -> None:\n        self._dag_stale_not_seen_duration = conf.getint(\n            \"scheduler\", \"dag_stale_not_seen_duration\"\n        )\n        dag = self.scheduler_dag_bag.load_dag(\n            dag_run=dag_run, session=self.session\n        )\n        dag.set_executor(self)\n\n    def _load_ti(self, ti: TI) -> None:\n        self._dag_st"
  },
  {
    "reference": "Handle all tracing requirements such as getting the tracer, and starting a new span.\n\n    When OTEL is enabled, the Trace class will be replaced by this class.",
    "prediction": "A span is an object that can be used to trace the execution of a piece of code. It is created by the\n    `otel_tracer` and can be used by `otel_tracer.start_span` or `otel_tracer.start_root_span`.\n    The `otel_tracer.start_span` method can be used to create child spans and the `otel_tracer.start_root_span`\n    method can be used to create a root span.\n    An instance of `otel_tracer` can be created by passing a `SpanExporter` and a `SpanProcessor` to the\n    constructor. The `SpanProcessor` is responsible for exporting spans at a specific interval and the\n    `SpanExporter` is responsible for exporting spans to the collector.\n    An `otel_tracer` can be used to create spans and trace the execution of a piece of code. It can be used\n    by passing a `SpanExporter` and a `SpanProcessor` to the constructor. The `SpanExporter` is responsible\n    for exporting spans to the collector and the `SpanProcessor` is responsible for exporting spans at a\n    specific interval.\n\"\"\"\n\nclass TraceContextTextMapPropagator:\n    \"\"\"\n    A `TraceContextTextMapPropagator` is"
  }
]