[
  {
    "reference": "A table for serialized DAGs.\n\n    serialized_dag table is a snapshot of DAG files synchronized by scheduler.\n    This feature is controlled by:\n\n    * ``[core] min_serialized_dag_update_interval = 30`` (s):\n      serialized DAGs are updated in DB when a file gets processed by scheduler,\n      to reduce DB write rate, there is a minimal interval of updating serialized DAGs.\n    * ``[dag_processor] refresh_interval = 300`` (s):\n      interval of deleting serialized DAGs in DB when the files are deleted, suggest\n      to use a smaller interval such as 60\n    * ``[core] compress_serialized_dags``:\n      whether compressing the dag data to the Database.\n\n    It is used by webserver to load dags\n    because reading from database is lightweight compared to importing from files,\n    it solves the webserver scalability issue.",
    "prediction": "This class is used to store serialized DAGs in the database. It inherits from `BaseModel`\nand implements the following methods:\n\n\\Only docstring:\n\n    :param dag: a DAG to be written into database\n    :param bundle_name: bundle name of the DAG\n    :param bundle_version: bundle version of the DAG\n    :param min_update_interval: minimal interval in seconds to update serialized DAG\n    :param session: ORM Session\n\n    :returns: Boolean indicating if the DAG was written to the DB\n\"\"\"\n\\Only docstring:\n\n    def hash(self):\n        \"\"\"Hash the data to get the dag_hash.\"\"\"\n        dag_data = self._data\n        data_json = json.dumps(dag_data, sort_keys=True).encode(\"utf-8\")\n        return md5(data_json).hexdigest()\n\n    def __init__(self, dag: DAG | LazyDeserializedDAG):\n        from airflow.sdk import DAG\n\n        self.dag_id = dag.dag_id\n        dag_data = {}\n        if isinstance(dag, DAG):\n            dag_data = SerializedDAG.to_dict(dag)\n        else:\n            dag_data = dag.data\n\n        self.dag_hash = SerializedDagModel.hash(dag_data)\n\n        # partially ordered json"
  },
  {
    "reference": "Lazy-loaded cached dictionary.\n\n    Dictionary, which in case you set callable, executes the passed callable with `key` attribute\n    at first use - and returns and caches the result.",
    "prediction": "`LazyDictWithCache` is a class that caches the resolved dictionary\n    (see `LazyDictWithCache._resolve()` and `LazyDictWithCache._raw()`).\n    It can be used instead of `dict` if you need a cache to resolve\n    a dictionary.\n\\Only docstring: only docstring\n\"\"\""
  },
  {
    "reference": "A dag (directed acyclic graph) is a collection of tasks with directional dependencies.\n\n    A dag also has a schedule, a start date and an end date (optional).  For each schedule,\n    (say daily or hourly), the DAG needs to run each individual tasks as their dependencies\n    are met. Certain tasks have the property of depending on their own past, meaning that\n    they can't run until their previous schedule (and upstream tasks) are completed.\n\n    DAGs essentially act as namespaces for tasks. A task_id can only be\n    added once to a DAG.\n\n    Note that if you plan to use time zones all the dates provided should be pendulum\n    dates. See :ref:`timezone_aware_dags`.\n\n    .. versionadded:: 2.4\n        The *schedule* argument to specify either time-based scheduling logic\n        (timetable), or asset-driven triggers.\n\n    .. versionchanged:: 3.0\n        The default value of *schedule* has been changed to *None* (no schedule).\n        The previous default was ``timedelta(days=1)``.\n\n    :param dag_id: The id of the DAG; must consist exclusively of alphanumeric\n        characters, dashes, dots and underscores (all ASCII)\n    :param description: The description for the DAG to e.g. be shown on the webserver\n    :param schedule: If provided, this defines the rules according to which DAG\n        runs are scheduled. Possible values include a cron expression string,\n        timedelta object, Timetable, or list of Asset objects.\n        See also :doc:`/howto/timetable`.\n    :param start_date: The timestamp from which the scheduler will\n        attempt to backfill. If this is not provided, backfilling must be done\n        manually with an explicit time range.\n    :param end_date: A date beyond which your DAG won't run, leave to None\n        for open-ended scheduling.\n    :param template_searchpath: This list of folders (non-relative)\n        defines where jinja will look for your templates. Order matters.\n        Note that jinja/airflow includes the path of your DAG file by\n        default\n    :param template_undefined: Template undefined type.\n    :param user_defined_macros: a dictionary of macros that will be exposed\n        in your jinja templates. For example, passing ``dict(foo='bar')``\n        to this argument allows you to ``{{ foo }}`` in all jinja\n        templates related to this DAG. Note that you can pass any\n        type of object here.\n    :param user_defined_filters: a dictionary of filters that will be exposed\n        in your jinja templates. For example, passing\n        ``dict(hello=lambda name: 'Hello %s' % name)`` to this argument allows\n        you to ``{{ 'world' | hello }}`` in all jinja templates related to\n        this DAG.\n    :param default_args: A dictionary of default parameters to be used\n        as constructor keyword parameters when initialising operators.\n        Note that operators have the same hook, and precede those defined\n        here, meaning that if your dict contains `'depends_on_past': True`\n        here and `'depends_on_past': False` in the operator's call\n        `default_args`, the actual value will be `False`.\n    :param params: a dictionary of DAG level parameters that are made\n        accessible in templates, namespaced under `params`. These\n        params can be overridden at the task level.\n    :param max_active_tasks: the number of task instances allowed to run\n        concurrently\n    :param max_active_runs: maximum number of active DAG runs, beyond this\n        number of DAG runs in a running state, the scheduler won't create\n        new active DAG runs\n    :param max_consecutive_failed_dag_runs: (experimental) maximum number of consecutive failed DAG runs,\n        beyond this the scheduler will disable the DAG\n    :param dagrun_timeout: Specify the duration a DagRun should be allowed to run before it times out or\n        fails. Task instances that are running when a DagRun is timed out will be marked as skipped.\n    :param sla_miss_callback: DEPRECATED - The SLA feature is removed in Airflow 3.0, to be replaced with a new implementation in 3.1\n    :param catchup: Perform scheduler catchup (or only run latest)? Defaults to False\n    :param on_failure_callback: A function or list of functions to be called when a DagRun of this dag fails.\n        A context dictionary is passed as a single parameter to this function.\n    :param on_success_callback: Much like the ``on_failure_callback`` except\n        that it is executed when the dag succeeds.\n    :param access_control: Specify optional DAG-level actions, e.g.,\n        \"{'role1': {'can_read'}, 'role2': {'can_read', 'can_edit', 'can_delete'}}\"\n        or it can specify the resource name if there is a DAGs Run resource, e.g.,\n        \"{'role1': {'DAG Runs': {'can_create'}}, 'role2': {'DAGs': {'can_read', 'can_edit', 'can_delete'}}\"\n    :param is_paused_upon_creation: Specifies if the dag is paused when created for the first time.\n        If the dag exists already, this flag will be ignored. If this optional parameter\n        is not specified, the global config setting will be used.\n    :param jinja_environment_kwargs: additional configuration options to be passed to Jinja\n        ``Environment`` for template rendering\n\n        **Example**: to avoid Jinja from removing a trailing newline from template strings ::\n\n            DAG(\n                dag_id=\"my-dag\",\n                jinja_environment_kwargs={\n                    \"keep_trailing_newline\": True,\n                    # some other jinja2 Environment options here\n                },\n            )\n\n        **See**: `Jinja Environment documentation\n        <https://jinja.palletsprojects.com/en/2.11.x/api/#jinja2.Environment>`_\n\n    :param render_template_as_native_obj: If True, uses a Jinja ``NativeEnvironment``\n        to render templates as native Python types. If False, a Jinja\n        ``Environment`` is used to render templates as string values.\n    :param tags: List of tags to help filtering DAGs in the UI.\n    :param owner_links: Dict of owners and their links, that will be clickable on the DAGs view UI.\n        Can be used as an HTTP link (for example the link to your Slack channel), or a mailto link.\n        e.g: {\"dag_owner\": \"https://airflow.apache.org/\"}\n    :param auto_register: Automatically register this DAG when it is used in a ``with`` block\n    :param fail_fast: Fails currently running tasks when task in DAG fails.\n        **Warning**: A fail fast dag can only have tasks with the default trigger rule (\"all_success\").\n        An exception will be thrown if any task in a fail fast dag has a non default trigger rule.\n    :param dag_display_name: The display name of the DAG which appears on the UI.",
    "prediction": "\\Only docstring docstring: DagRun.handle_callback.\n\\Only docstring docstring: DagRun.handle_callback.\n\n\\Only docstring docstring: DagRun.handle_callback.\n\\Only docstring docstring: DagRun.handle_callback.\n\\Only docstring docstring: DagRun.handle_callback.\n\\Only docstring docstring: DagRun.handle_callback.\n\n\\Only docstring docstring: DagRun.handle_callback.\n\\Only docstring docstring: DagRun.handle_callback.\n\n\\Only docstring docstring: DagRun.handle_callback.\n\\Only docstring docstring: DagRun.handle_callback.\n\n\\Only docstring docstring: DagRun.handle_callback.\n\\Only docstring docstring: DagRun.handle_callback.\n\n\\Only docstring docstring: DagRun.handle_callback.\n\\Only docstring docstring: DagRun.handle_callback.\n\n\\Only docstring docstring: DagRun.handle_callback.\n\\Only docstring docstring: DagRun.handle_callback.\n\n\\Only docstring docstring: DagRun.handle_callback.\n\\Only docstring docstring: DagRun.handle_callback.\n\n\\Only docstring docstring: DagRun.handle_callback.\n\\Only docstring docstring: DagRun.handle_callback.\n\n\\Only docstring docstring: DagRun.handle_callback.\n\\Only doc"
  },
  {
    "reference": "States that a Task Instance can be in that indicate it is not yet in a terminal or running state.",
    "prediction": "The following class represents a state that an intermediate task is in.\n\nIt is used as a state in the workflow to represent the state of the intermediate task.\n\\Only docstring: The following class represents a state that an intermediate task is in.\nIt is used as a state in the workflow to represent the state of the intermediate task.\n\"\"\"\n\nclass IntermediateTIState(IntermediateTIState):\n    \"\"\"\n    This class represents a state that an intermediate task is in.\n\n    It is used as a state in the workflow to represent the state of the intermediate task.\n    \"\"\"\n\n    def __init__(self, state: IntermediateTIState):\n        \"\"\"\n        Initialize the state.\n\n        :param state: The state to initialize.\n        \"\"\"\n        super().__init__(state)\n        self.state = state\n\n    @classmethod\n    def from_dict(cls, d: dict[str, Any]) -> \"IntermediateTIState\":\n        \"\"\"\n        Create a state from a dictionary.\n\n        :param d: The dictionary to create the state from.\n        :return: The created state.\n        \"\"\"\n        return cls(d[\"state\"])\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Create a dictionary from the state.\n\n        :return: The dictionary representation of the state.\n        \"\"\""
  },
  {
    "reference": "Exception raised when a model populates data interval fields incorrectly.\n\n    The data interval fields should either both be None (for runs scheduled\n    prior to AIP-39), or both be datetime (for runs scheduled after AIP-39 is\n    implemented). This is raised if exactly one of the fields is None.",
    "prediction": "A `data_interval` is a data interval that is not a valid datetime.\n\nA data interval is only a valid datetime if it is either None or a datetime.\n\"\"\""
  },
  {
    "reference": "Raise when the task should be re-scheduled at a later time.\n\n    :param reschedule_date: The date when the task should be rescheduled",
    "prediction": "This exception is raised when an Airflow operator is rescheduled. The\nreschedule date is the date on which the reschedule is triggered.\n\nThe reschedule date is the date on which the reschedule is triggered, not the\ndate on which the operator is actually rescheduled.\n\"\"\""
  },
  {
    "reference": "Manages all provider distributions.\n\n    This is a Singleton class. The first time it is\n    instantiated, it discovers all available providers in installed packages.",
    "prediction": "This class is responsible for managing the Airflow provider information.\nIt is a singleton class, so it should be initialized only once.\n\"\"\"\n\n    @classmethod\n    def initialize_providers(cls):\n        \"\"\"Initialize the Airflow provider information.\"\"\"\n        cls.initialize_providers_list()\n        cls.initialize_providers_hooks()\n        cls.initialize_providers_filesystems()\n        cls.initialize_providers_asset_uri_resources()\n        cls.initialize_providers_taskflow_decorators()\n        cls.initialize_providers_extra_links()\n        cls.initialize_providers_logging()\n        cls.initialize_providers_secrets_backends()\n        cls.initialize_providers_executors()\n        cls.initialize_providers_notifications()\n        cls.initialize_providers_auth_managers()\n        cls.initialize_providers_configuration()\n        cls.initialize_providers_plugins()\n        cls.initialize_providers_configuration()\n        cls.initialize_providers_hooks()\n\n    def get_connection_info(self, connection_type: str) -> ConnectionInfo:\n        \"\"\"Retrieve connection information.\"\"\"\n        return self._connection_form_widgets.get(connection_type)\n\n    def get_field_behaviour(self, field: FieldInfo) -> dict[str, str]:\n        \"\"\"Retrieve field behaviour.\"\"\"\n        return self._field_behaviours.get(field.name)\n\n    def get_extra_link_class_name(self) -> str:\n        \"\"\"Retrieve extra link class name.\"\"\"\n        return"
  },
  {
    "reference": "Table defining different owner attributes.\n\n    For example, a link for an owner that will be passed as a hyperlink to the \"DAGs\" view.",
    "prediction": "This class represents an attribute of a DagOwner.\n\"\"\""
  },
  {
    "reference": "Store info needed to create XComArg.\n\n    We can't turn it in to a XComArg until we've loaded _all_ the tasks, so when\n    deserializing an operator, we need to create something in its place, and\n    post-process it in ``deserialize_dag``.",
    "prediction": "The class XComRef represents an xcom ref that has been deserialized from an\nxcom arg.\n\"\"\""
  },
  {
    "reference": "Holds modifications to be applied when writing out the config.\n\n    :param rename: Mapping from (old_section, old_option) to (new_section, new_option)\n    :param remove: Set of (section, option) to remove\n    :param default_updates: Mapping from (section, option) to new default value",
    "prediction": "This class contains configuration modifications that are only applied to a single section of the config.\n\"\"\"\n\n    def __init__(self) -> None:\n        self.rename: dict[tuple[str, str], tuple[str, str]] = {}\n        self.remove: set[tuple[str, str]] = set()\n        self.default_updates: dict[tuple[str, str], str] = {}\n\n    def add_rename(self, old_section: str, old_option: str, new_section: str, new_option: str) -> None:\n        self.rename[(old_section, old_option)] = (new_section, new_option)\n\n    def add_remove(self, section: str, option: str) -> None:\n        self.remove.add((section, option))\n\n    def add_default_update(self, section: str, option: str, new_default: str) -> None:\n        self.default_updates[(section, option)] = new_default\n\nclass DefaultConfigModifications:\n    def __init__(self) -> None:\n        self.default_updates: dict[str, str] = {}\n\n    def add_default_update(self, section: str, option: str, new_default: str) -> None:\n        self.default_updates[(section, option)] = new_default\n\n\\Only docstring:\nclass Section"
  },
  {
    "reference": "Timetable that triggers DAG runs according to a cron expression.\n\n    This is different from ``CronDataIntervalTimetable``, where the cron\n    expression specifies the *data interval* of a DAG run. With this timetable,\n    the data intervals are specified independently from the cron expression.\n    Also for the same reason, this timetable kicks off a DAG run immediately at\n    the start of the period (similar to POSIX cron), instead of needing to wait\n    for one data interval to pass.\n\n    Don't pass ``@once`` in here; use ``OnceTimetable`` instead.\n\n    :param cron: cron string that defines when to run\n    :param timezone: Which timezone to use to interpret the cron string\n    :param interval: timedelta that defines the data interval start. Default 0.\n\n    *run_immediately* controls, if no *start_time* is given to the DAG, when\n    the first run of the DAG should be scheduled. It has no effect if there\n    already exist runs for this DAG.\n\n    * If *True*, always run immediately the most recent possible DAG run.\n    * If *False*, wait to run until the next scheduled time in the future.\n    * If passed a ``timedelta``, will run the most recent possible DAG run\n      if that run's ``data_interval_end`` is within timedelta of now.\n    * If *None*, the timedelta is calculated as 10% of the time between the\n      most recent past scheduled time and the next scheduled time. E.g. if\n      running every hour, this would run the previous time if less than 6\n      minutes had past since the previous run time, otherwise it would wait\n      until the next hour.",
    "prediction": "Timetable class to represent a time-based trigger.\n\"\"\"\nfrom airflow.models.base_model import BaseModel\nfrom airflow.models.base_model import BaseModelMixin\nfrom airflow.utils import timezone\nfrom airflow.utils import timezone\nfrom airflow.utils import timezone\n\nclass TriggerTimetable(BaseModelMixin, Timetable):\n    \"\"\"\n    Timetable class to represent a time-based trigger.\n    \"\"\"\n\n    def __init__(\n        self,\n        cron: str,\n        *,\n        timezone: str | Timezone | FixedTimezone,\n        interval: datetime.timedelta | relativedelta = datetime.timedelta(),\n        run_immediately: bool | datetime.timedelta = False,\n    ) -> None:\n        super().__init__(cron, timezone)\n        self._interval = interval\n        self._run_immediately = run_immediately\n\n    def serialize(self) -> dict[str, Any]:\n        from airflow.serialization.serialized_objects import encode_timezone\n\n        return {\n            \"expression\": self._expression,\n            \"timezone\": encode_timezone(self._timezone),\n            \"interval\": _serialize_interval(self._interval),\n            \"run_immediately\": _serialize_run_immediately(self._run_immediately),\n        }\n\n    def deserialize(self, data: dict[str, Any]) -> Timetable:\n        from airflow.serialization.serialized_objects import decode_timezone"
  },
  {
    "reference": "Collection of active assets.\n\n    An asset is considered active if it is declared by the user in any DAG files.\n    AssetModel entries that are not active (also called orphaned in some parts\n    of the code base) are still kept in the database, but have their corresponding\n    entries in this table removed. This ensures we keep all possible history on\n    distinct assets (those with non-matching name-URI pairs), but still ensure\n    *name and URI are each unique* within active assets.",
    "prediction": "AssetActiveModel is a model that defines the asset active table.\n\"\"\""
  },
  {
    "reference": "A base class for dependency contexts.\n\n    Specifies which dependencies should be evaluated in the context for a task\n    instance to satisfy the requirements of the context. Also stores state\n    related to the context that can be used by dependency classes.\n\n    For example there could be a SomeRunContext that subclasses this class which has\n    dependencies for:\n\n    - Making sure there are slots available on the infrastructure to run the task instance\n    - A task-instance's task-specific dependencies are met (e.g. the previous task\n      instance completed successfully)\n    - ...\n\n    :param deps: The context-specific dependencies that need to be evaluated for a\n        task instance to run in this execution context.\n    :param flag_upstream_failed: This is a hack to generate the upstream_failed state\n        creation while checking to see whether the task instance is runnable. It was the\n        shortest path to add the feature. This is bad since this class should be pure (no\n        side effects).\n    :param ignore_all_deps: Whether or not the context should ignore all ignorable\n        dependencies. Overrides the other ignore_* parameters\n    :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs (e.g. for\n        Backfills)\n    :param wait_for_past_depends_before_skipping: Wait for past depends before marking the ti as skipped\n    :param ignore_in_retry_period: Ignore the retry period for task instances\n    :param ignore_in_reschedule_period: Ignore the reschedule period for task instances\n    :param ignore_unmapped_tasks: Ignore errors about mapped tasks not yet being expanded\n    :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past and\n        trigger rule\n    :param ignore_ti_state: Ignore the task instance's previous failure/success\n    :param finished_tis: A list of all the finished task instances of this run",
    "prediction": "This class represents the dependency context used to evaluate dependencies and\nthe context in which to run tasks.  This class is used in two contexts:\n\\subheading{Dependency Evaluation}\n\\onlyDocstring{The dependency evaluation context is used to evaluate dependencies\nbetween tasks and to determine whether a task should be run.\n}\n\n\\subheading{Task Execution Context}\n\\onlyDocstring{The task execution context is used to determine which tasks to run\nand which dependencies to evaluate.\n}\n\"\"\"\n\n    def __init__(self, session: Session):\n        self.session = session\n        self.dag_run = DagRun(session=self.session)\n        self.dag_run.session = session\n\n    def __str__(self) -> str:\n        return f\"{self.__class__.__name__}({self.dag_run})\"\n\n    def __repr__(self) -> str:\n        return self.__str__()\n\n    def __eq__(self, other: object | None) -> bool:\n        return isinstance(other, self.__class__) and self.dag_run == other.dag_run\n\n    def __ne__(self, other: object | None) -> bool:\n        return not self.__eq__(other)\n\n    def __hash__(self) -> int:\n        return hash((self"
  },
  {
    "reference": "Request for DAG File Parsing.\n\n    This is the request that the manager will send to the DAG parser with the dag file and\n    any other necessary metadata.",
    "prediction": "This model describes a request to parse a DagFile.\n\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\Only docstring\\"
  },
  {
    "reference": "Serializer for responses to bulk entity operations.\n\n    This represents the results of create, update, and delete actions performed on entity in bulk.\n    Each action (if requested) is represented as a field containing details about successful keys and any encountered errors.\n    Fields are populated in the response only if the respective action was part of the request, else are set None.",
    "prediction": "A BulkActionResponse contains the details of a bulk action operation.  It contains the following fields:\n\n- `create`: `BulkActionResponse` containing details of the bulk create operation, including successful keys and errors.\n- `update`: `BulkActionResponse` containing details of the bulk update operation, including successful keys and errors.\n- `delete`: `BulkActionResponse` containing details of the bulk delete operation, including successful keys and errors.\n- `error`: `BulkError` containing details of the bulk error operation.\n\"\"\"\nclass BulkActionResponse(BaseModel):\n    \"\"\"\n    A `BulkActionResponse` contains the details of a bulk action operation.  It contains the following fields:\n\n    - `create`: `BulkActionResponse` containing details of the bulk create operation, including successful keys and errors.\n    - `update`: `BulkActionResponse` containing details of the bulk update operation, including successful keys and errors.\n    - `delete`: `BulkActionResponse` containing details of the bulk delete operation, including successful keys and errors.\n    - `error`: `BulkError` containing details of the bulk error operation.\n    \"\"\"\n\n    key: str\n    message: str | None\n    error: BulkError | None = None\n    status_code: int"
  },
  {
    "reference": "TriggerRunnerSupervisor is responsible for monitoring the subprocess and marshalling DB access.\n\n    This class (which runs in the main process) is responsible for querying the DB, sending RunTrigger\n    workload messages to the subprocess, and collecting results and updating them in the DB.",
    "prediction": "The TriggerRunner class is responsible for running and maintaining a Trigger\nand its associated trigger jobs.\n\nTrigger jobs are triggered by a trigger run job, and are executed by a trigger\nrunner. Trigger jobs are executed by a trigger run job. Trigger jobs are executed\nby a trigger run job. Trigger jobs are executed by a trigger run job.\n\nTrigger jobs are triggered by a trigger run job, and are executed by a trigger\nrunner. Trigger jobs are executed by a trigger run job. Trigger jobs are executed\nby a trigger run job. Trigger jobs are executed by a trigger run job.\n\nTrigger jobs are triggered by a trigger run job, and are executed by a trigger\nrunner. Trigger jobs are executed by a trigger run job. Trigger jobs are executed\nby a trigger run job. Trigger jobs are executed by a trigger run job.\n\nTrigger jobs are triggered by a trigger run job, and are executed by a trigger\nrunner. Trigger jobs are executed by a trigger run job. Trigger jobs are executed\nby a trigger run job. Trigger jobs are executed by a trigger run job.\n\nTrigger jobs are triggered by a trigger run job, and are executed by a trigger\nrunner. Trigger jobs are executed by a trigger run job. Trigger jobs are executed\nby a trigger"
  },
  {
    "reference": "An implementation of Stats.Timer() which records the result in the OTel Metrics Map.\n\n    OpenTelemetry does not have a native timer, we will store the values as a Gauge.\n\n    :param name: The name of the timer.\n    :param tags: Tags to append to the timer.",
    "prediction": "This class is a base class for timers which are run by the otel_timer module.\n\"\"\"\nclass OtelTimer(Timer):\n    def __init__(self, otel_logger: SafeOtelLogger, name: str | None, tags: Attributes):\n        super().__init__()\n        self.otel_logger = otel_logger\n        self.name = name\n        self.tags = tags\n\n    def stop(self, send: bool = True) -> None:\n        super().stop(send)\n        self.otel_logger.metrics_map.set_gauge_value(\n            full_name(prefix=self.otel_logger.prefix, name=self.name), self.duration, False, self.tags\n        )\n\n    def start(self, duration: float, send: bool = True) -> None:\n        super().start(duration, send)\n        self.otel_logger.metrics_map.set_gauge_value(\n            full_name(prefix=self.otel_logger.prefix, name=self.name), duration, True, self.tags\n        )\n\n    def stop(self, send: bool = True) -> None:\n        super().stop(send)\n        self.otel_logger.metrics_map.set_gauge_value(\n            full_name(prefix=self.otel_logger.prefix, name=self.name), self.duration, False, self.tags\n        )\n\n    def start(self, duration:"
  },
  {
    "reference": "Ensures that the task instance's state is in a given set of valid states.\n\n    :param valid_states: A list of valid states that a task instance can have to meet\n        this dependency.\n    :return: whether or not the task instance's state is valid",
    "prediction": "A task instance dependency is a stateful dependency that can only be set by an existing task instance.\nIt only needs to be set once.\n\"\"\"\n\nclass ValidStateDep(BaseTIDep):\n    \"\"\"Task Instance State Dependency\"\"\"\n    NAME = \"Task Instance State\"\n    IGNORABLE = True\n    def __init__(self, valid_states):\n        super().__init__()\n        if not valid_states:\n            raise AirflowException(\"ValidStatesDep received an empty set of valid states.\")\n        self._valid_states = valid_states\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"Check if two task instance dependencies are equal by comparing their types and valid states.\"\"\"\n        if not isinstance(other, ValidStateDep):\n            return NotImplemented\n        return self._valid_states == other._valid_states\n\n    def __hash__(self):\n        \"\"\"Compute the hash value based on the type of the task instance dependency and its valid states.\"\"\"\n        return hash((type(self), tuple(self._valid_states)))\n\n    @provide_session\n    def _get_dep_statuses(self, ti, session, dep_context):\n        if dep_context.ignore_ti_state:\n            yield self._passing_status(reason=\"Context specified that state should be ignored.\")\n            return\n\n        if ti"
  },
  {
    "reference": "The resources required by an operator.\n\n    Resources that are not specified will use the default values from the airflow config.\n\n    :param cpus: The number of cpu cores that are required\n    :param ram: The amount of RAM required\n    :param disk: The amount of disk space required\n    :param gpus: The number of gpu units that are required",
    "prediction": "Resource objects that are used to represent the hardware resources.\n    These resource objects are used to define the resource type in the\n    resource definition.\n\"\"\"\n\nclass CpuResource(RelativeResource):\n    \"\"\"A CPU resource.\"\"\"\n\n    def __init__(self, cpus: int | None):\n        self.cpus = cpus\n        super().__init__()\n\n    def to_dict(self) -> dict:\n        return {\n            \"qty\": self.cpus,\n            \"type\": \"cpu\",\n        }\n\n    @classmethod\n    def from_dict(cls, resource_dict: dict) -> CpuResource:\n        return cls(resource_dict[\"qty\"])\n\n    def is_available(self) -> bool:\n        return self.cpus > 0\n\n    def is_unavailable(self) -> bool:\n        return self.cpus <= 0\n\n    def is_reserved(self) -> bool:\n        return self.cpus == 0\n\n    def is_available_for(self, task: Task) -> bool:\n        return self.is_available()\n\n    def is_unavailable_for(self, task: Task) -> bool:\n        return self.is_unavailable()\n\n    def is_reserved_for(self, task: Task) -> bool:\n        return self.is_reserved()\n\n    def get_available_for(self, task:"
  },
  {
    "reference": "Timetable that schedules data intervals with a cron expression.\n\n    This corresponds to ``schedule=<cron>``, where ``<cron>`` is either\n    a five/six-segment representation, or one of ``cron_presets``.\n\n    The implementation extends on croniter to add timezone awareness. This is\n    because croniter works only with naive timestamps, and cannot consider DST\n    when determining the next/previous time.\n\n    Don't pass ``@once`` in here; use ``OnceTimetable`` instead.",
    "prediction": "This is a simplified representation of a data interval that is created based\non the current date and time. The `data_interval` class represents a data interval\nas a set of time periods (start, end) that should be run.\n\"\"\"\nfrom airflow.models import BaseModel\nfrom airflow.utils import timezone\nimport datetime\n\nfrom airflow.utils import timezone\nfrom airflow.utils import time\n\nfrom airflow.utils import time\n\nimport airflow.models\nfrom airflow.utils import timezone\nimport airflow.utils.time\n\n\ndef _data_interval_infer(run_after: time.Time) -> DataInterval:\n    \"\"\"Infer the data interval based on the current date and time.\n\n    This method should be called only if we have a manual data interval\n    that is not based on the current date and time, e.g. the data interval\n    that was created when a DAG run was triggered manually.\n    \"\"\"\n    # Get the last complete period before run_after, e.g. if a DAG run is\n    # scheduled at each midnight, the data interval of a manually triggered\n    # run at 1am 25th is between 0am 24th and 0am 25th.\n    end = _data_interval_infer_incr(run_after)\n    return DataInterval(start"
  },
  {
    "reference": "Base Trigger class.\n\n    Triggers are a workload that run in an asynchronous event loop shared with\n    other Triggers, and fire off events that will unpause deferred Tasks,\n    start linked DAGs, etc.\n\n    They are persisted into the database and then re-hydrated into a\n    \"triggerer\" process, where many are run at once. We model it so that\n    there is a many-to-one relationship between Task and Trigger, for future\n    deduplication logic to use.\n\n    Rows will be evicted from the database when the triggerer detects no\n    active Tasks/DAGs using them. Events are not stored in the database;\n    when an Event is fired, the triggerer will directly push its data to the\n    appropriate Task/DAG.",
    "prediction": "Trigger is the only model that can be directly serialized as JSON, so it's\nthe only model that can be used as a trigger payload in a JSON payload.\n\"\"\""
  },
  {
    "reference": "Lazily build information from the serialized DAG structure.\n\n    An object that will present \"enough\" of the DAG like interface to update DAG db models etc, without having\n    to deserialize the full DAG and Task hierarchy.",
    "prediction": "The DAG class defines a single DAG object with one or more tasks.\nThe DAG class also has several special properties that are only available when the DAG is deserialized from a serialized file.\n\nThe serialized DAG object only contains the following properties:\n\n    dag_id: The id of the DAG. This is only used when the DAG is deserialized from a serialized file. The DAG id is used to look up the DAG object in the database.\n    dag_display_name: The display name of the DAG. The display name is only used when the DAG is deserialized from a serialized file. The display name is used to display the DAG in the UI.\n    dag_run_id: The id of the current DAG run. This is only used when the DAG is deserialized from a serialized file. The DAG run id is used to look up the DAG run in the database.\n    dag_run_number: The number of the current DAG run. This is only used when the DAG is deserialized from a serialized file. The DAG run number is used to look up the DAG run in the database.\n    dag_version: The version of the DAG. This is only used when the DAG is deserialized from a serialized file. The DAG version is used to look up the DAG object in the database"
  },
  {
    "reference": "An extension of FileHandler, advises the Kernel to not cache the file in PageCache when it is written.\n\n    While there is nothing wrong with such cache (it will be cleaned when memory is needed), it\n    causes ever-growing memory usage when scheduler is running as it keeps on writing new log\n    files and the files are not rotated later on. This might lead to confusion for our users,\n    who are monitoring memory usage of Scheduler - without realising that it is harmless and\n    expected in this case.\n\n    See https://github.com/apache/airflow/issues/14924\n\n    Adding the advice to Kernel might help with not generating the cache memory growth in the first place.",
    "prediction": "A file handler that doesn't cache its output, for use with\n``.getoutput`` and ``.getstatusoutput``.\n\"\"\""
  },
  {
    "reference": "SchedulerJobRunner runs for a specific time interval and schedules jobs that are ready to run.\n\n    It figures out the latest runs for each task and sees if the dependencies\n    for the next schedules are met.\n    If so, it creates appropriate TaskInstances and sends run commands to the\n    executor. It does this for each task in each DAG and repeats.\n\n    :param num_runs: The number of times to run the scheduling loop. If you\n        have a large number of DAG files this could complete before each file\n        has been parsed. -1 for unlimited times.\n    :param scheduler_idle_sleep_time: The number of seconds to wait between\n        polls of running processors\n    :param log: override the default Logger",
    "prediction": "This class is the base class for all SchedulerJobRunner subclasses. The job runner will\nattempt to execute the tasks in the given executor(s) for the given DAG(s). If the DAG(s)\nare stale, it will attempt to re-run the DAG(s) using the same executor(s).\nIf the DAG(s) are stale and have been re-run, it will attempt to re-run them with a different\nexecutor(s). If the DAG(s) are stale and have been re-run with a different executor(s), it\nwill attempt to re-run them with the same executor(s).\n\nIf the DAG(s) are stale, the job runner will attempt to re-run them using the same executor(s)\nwith the same DAG run ID and task ID. If the DAG(s) are stale and have been re-run with the same\nexecutor(s) with the same DAG run ID and task ID, it will attempt to re-run them with a\ndifferent executor(s) with the same DAG run ID and task ID.\n\nIf the DAG(s) are stale, the job runner will attempt to re-run them using the same executor(s)\nwith the same DAG run ID and task ID. If the DAG(s) are stale and have been re-run with the same\nexecutor(s)"
  },
  {
    "reference": "Handle all tracing requirements such as getting the tracer, and starting a new span.\n\n    When OTEL is enabled, the Trace class will be replaced by this class.",
    "prediction": "This class is a wrapper around the AirflowOtelIdGenerator to control producing certain span and trace id.\n\nIt can be used to get a tracer and directly create spans, or for auto-instrumentation.\n\"\"\"\nclass TracerProvider:\n    \"\"\"\n    A TracerProvider is a Tracer that is configured with an ID generator and a TraceContextTextMapPropagator.\n    It can be used to get a Tracer and directly create spans, or for auto-instrumentation.\n\n    When using TracerProvider, the SpanContext is automatically injected into the carrier.\n\n    The SpanContext is only injected if the span is created from the context of the current active span.\n    This is done to avoid injecting the span context into the carrier if the span is created from the context\n    of a child span, because the child span context will point to the span context of the child span.\n    \"\"\"\n\n    def __init__(self, resource: Resource) -> None:\n        self.resource = resource\n        self.id_generator: IDGenerator = AirflowOtelIdGenerator(span_id=INVALID_SPAN_ID, trace_id=INVALID_TRACE_ID)\n        self._trace_context_text_map_propagator: TraceContextTextMapPropagator = TraceContextTextMapPropag"
  }
]